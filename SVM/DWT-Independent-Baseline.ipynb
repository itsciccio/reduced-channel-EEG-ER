{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "brief-second",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pickle\n",
    "import os\n",
    "import time\n",
    "from pywt import wavedec\n",
    "import pyeeg\n",
    "import scipy.io as sio\n",
    "\n",
    "from pathlib import Path\n",
    "cwd = os.getcwd()\n",
    "parent = Path(cwd).parent\n",
    "\n",
    "from importlib.machinery import SourceFileLoader\n",
    "thundersvm = SourceFileLoader(\"thundersvm\", r\"thundersvm\\python\\thundersvm\\thundersvm.py\").load_module()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fancy-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywt import Wavelet\n",
    "from math import floor, ceil\n",
    "from numpy import concatenate, flipud, zeros, convolve, array\n",
    "\n",
    "def padding_symmetric(signal, size=8):\n",
    "    '''\n",
    "    Applies a symmetric padding of the specified size to the input signal.\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : ndarray\n",
    "        The signal to be padded.\n",
    "    size : int, optional\n",
    "        The size of the padding which corresponds to the size of the filter. The default is 8.\n",
    "    Returns\n",
    "    -------\n",
    "    padded_signal : ndarray\n",
    "        Padded signal.\n",
    "    '''\n",
    "    \n",
    "    padded_signal = concatenate([flipud(signal[:size]), signal, flipud(signal[-size:])])\n",
    "    return padded_signal\n",
    "\n",
    "\n",
    "def restore_signal(signal, reconstruction_filter, real_len):\n",
    "    '''\n",
    "    Restores the signal to its original size using the reconstruction filter.\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : ndarray\n",
    "        The signal to be restored.\n",
    "    reconstruction_filter : list\n",
    "        The reconstruction filter to be used for restoring the signal.\n",
    "    real_len : int\n",
    "        Real length of the signal.\n",
    "    Returns\n",
    "    -------\n",
    "    restored_signal : ndarray\n",
    "        Restored signal of the specified length.\n",
    "    '''\n",
    "    restored_signal = zeros(2 * len(signal) + 1)\n",
    "    for i in range(len(signal)):\n",
    "        restored_signal[i*2+1] = signal[i]\n",
    "    restored_signal = convolve(restored_signal, reconstruction_filter)\n",
    "    restored_len = len(restored_signal)\n",
    "    exceed_len = (restored_len - real_len) / 2\n",
    "    restored_signal = restored_signal[int(floor(exceed_len)):(restored_len - int(ceil(exceed_len)))]\n",
    "    return restored_signal\n",
    "\n",
    "def DWTfn(signal, level=3, mother_wavelet='db4'):\n",
    "    '''\n",
    "    Applies a Discrete Wavelet Transform to the signal.\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : ndarray\n",
    "        The signal on which the DWT will be applied.\n",
    "    level : int, optional\n",
    "        The decomposition levels for the DWT. The default is 3.\n",
    "    mother_wavelet : str, optional\n",
    "        The mother wavelet that it is going to be used in the DWT. The default is \"db4\".\n",
    "    Returns\n",
    "    -------\n",
    "    restored_approx_coeff : list\n",
    "        Restored approximations coefficients.\n",
    "    restored_detail_coeff : list\n",
    "        Restored detail coefficients.\n",
    "    '''\n",
    "    if type(signal).__name__ != \"ndarray\" and type(signal) != list:\n",
    "        raise TypeError(f\"'signal' must be 'ndarray', received: '{type(signal).__name__}'\")\n",
    "    if type(signal) == list:\n",
    "        signal = array(signal)\n",
    "    if \"float\" not in signal.dtype.name and \"int\" not in signal.dtype.name:\n",
    "        raise TypeError(f\"All elements of 'signal' must be numbers\")\n",
    "           \n",
    "    if type(level) != int:\n",
    "        raise TypeError(f\"'level' must be 'int', received: '{type(level).__name__}'\")\n",
    "    if level < 1:\n",
    "        raise TypeError(f\"'level' must be greater than 0, received: {level}\")\n",
    "        \n",
    "    if mother_wavelet not in ['haar', 'db1', 'db2', 'db3', 'db4', 'db5', 'db6', 'db7', 'db8', 'db9', 'db10', 'db11', 'db12', 'db13', 'db14', 'db15', 'db16', 'db17', 'db18', 'db19', 'db20', 'db21', 'db22', 'db23', 'db24', 'db25', 'db26', 'db27', 'db28', 'db29', 'db30', 'db31', 'db32', 'db33', 'db34', 'db35', 'db36', 'db37', 'db38', 'sym2', 'sym3', 'sym4', 'sym5', 'sym6', 'sym7', 'sym8', 'sym9', 'sym10', 'sym11', 'sym12', 'sym13', 'sym14', 'sym15', 'sym16', 'sym17', 'sym18', 'sym19', 'sym20', 'coif1', 'coif2', 'coif3', 'coif4', 'coif5', 'coif6', 'coif7', 'coif8', 'coif9', 'coif10', 'coif11', 'coif12', 'coif13', 'coif14', 'coif15', 'coif16', 'coif17', 'bior1.1', 'bior1.3', 'bior1.5', 'bior2.2', 'bior2.4', 'bior2.6', 'bior2.8', 'bior3.1', 'bior3.3', 'bior3.5', 'bior3.7', 'bior3.9', 'bior4.4', 'bior5.5', 'bior6.8', 'rbio1.1', 'rbio1.3', 'rbio1.5', 'rbio2.2', 'rbio2.4', 'rbio2.6', 'rbio2.8', 'rbio3.1', 'rbio3.3', 'rbio3.5', 'rbio3.7', 'rbio3.9', 'rbio4.4', 'rbio5.5', 'rbio6.8', 'dmey', 'gaus1', 'gaus2', 'gaus3', 'gaus4', 'gaus5', 'gaus6', 'gaus7', 'gaus8', 'mexh', 'morl', 'cgau1', 'cgau2', 'cgau3', 'cgau4', 'cgau5', 'cgau6', 'cgau7', 'cgau8', 'shan', 'fbsp', 'cmor']:\n",
    "        raise TypeError(f\"Invalid 'mother_wavelet' must be 'haar', 'db1', 'db2', 'db3', 'db4', 'db5', 'db6', 'db7', 'db8', 'db9', 'db10', 'db11', 'db12', 'db13', 'db14', 'db15', 'db16', 'db17', 'db18', 'db19', 'db20', 'db21', 'db22', 'db23', 'db24', 'db25', 'db26', 'db27', 'db28', 'db29', 'db30', 'db31', 'db32', 'db33', 'db34', 'db35', 'db36', 'db37', 'db38', 'sym2', 'sym3', 'sym4', 'sym5', 'sym6', 'sym7', 'sym8', 'sym9', 'sym10', 'sym11', 'sym12', 'sym13', 'sym14', 'sym15', 'sym16', 'sym17', 'sym18', 'sym19', 'sym20', 'coif1', 'coif2', 'coif3', 'coif4', 'coif5', 'coif6', 'coif7', 'coif8', 'coif9', 'coif10', 'coif11', 'coif12', 'coif13', 'coif14', 'coif15', 'coif16', 'coif17', 'bior1.1', 'bior1.3', 'bior1.5', 'bior2.2', 'bior2.4', 'bior2.6', 'bior2.8', 'bior3.1', 'bior3.3', 'bior3.5', 'bior3.7', 'bior3.9', 'bior4.4', 'bior5.5', 'bior6.8', 'rbio1.1', 'rbio1.3', 'rbio1.5', 'rbio2.2', 'rbio2.4', 'rbio2.6', 'rbio2.8', 'rbio3.1', 'rbio3.3', 'rbio3.5', 'rbio3.7', 'rbio3.9', 'rbio4.4', 'rbio5.5', 'rbio6.8', 'dmey', 'gaus1', 'gaus2', 'gaus3', 'gaus4', 'gaus5', 'gaus6', 'gaus7', 'gaus8', 'mexh', 'morl', 'cgau1', 'cgau2', 'cgau3', 'cgau4', 'cgau5', 'cgau6', 'cgau7', 'cgau8', 'shan', 'fbsp', or 'cmor', received: '{mother_wavelet}'\")\n",
    "        \n",
    "    original_len = len(signal)\n",
    "    approx_coeff = []\n",
    "    detail_coeff = []\n",
    "    wavelet = pywt.Wavelet(mother_wavelet)\n",
    "    low_filter = wavelet.dec_lo\n",
    "    high_filter = wavelet.dec_hi\n",
    "    filter_size = len(low_filter)\n",
    "    try:\n",
    "        for _ in range(level):\n",
    "            padded_signal = padding_symmetric(signal, filter_size)\n",
    "            low_pass_filtered_signal = convolve(padded_signal, low_filter)[filter_size:(2*filter_size)+len(signal)-1] \n",
    "            low_pass_filtered_signal = low_pass_filtered_signal[1:len(low_pass_filtered_signal):2]\n",
    "            high_pass_filtered_signal = convolve(padded_signal, high_filter)[filter_size:filter_size+len(signal)+filter_size-1]\n",
    "            high_pass_filtered_signal = high_pass_filtered_signal[1:len(high_pass_filtered_signal):2]\n",
    "            approx_coeff.append(low_pass_filtered_signal)\n",
    "            detail_coeff.append(high_pass_filtered_signal)\n",
    "            signal = low_pass_filtered_signal\n",
    "    except:\n",
    "        raise\n",
    "    low_reconstruction_filter = wavelet.rec_lo\n",
    "    high_reconstruction_filter = wavelet.rec_hi\n",
    "    real_lengths = []\n",
    "    for i in range(level-2,-1,-1):\n",
    "        real_lengths.append(len(approx_coeff[i]))\n",
    "    real_lengths.append(original_len)\n",
    "    restored_approx_coeff = []\n",
    "    for i in range(level):\n",
    "        restored_signal = restore_signal(approx_coeff[i], low_reconstruction_filter, real_lengths[level-1-i])\n",
    "        for j in range(i):\n",
    "            restored_signal = restore_signal(restored_signal, low_reconstruction_filter, real_lengths[level-i+j])\n",
    "        restored_approx_coeff.append(restored_signal)\n",
    "    restored_detail_coeff = []\n",
    "    for i in range(level):\n",
    "        restored_signal = restore_signal(detail_coeff[i], high_reconstruction_filter, real_lengths[level-1-i])\n",
    "        for j in range(i):\n",
    "            restored_signal = restore_signal(restored_signal, high_reconstruction_filter, real_lengths[level-i+j])\n",
    "        restored_detail_coeff.append(restored_signal)\n",
    "    return restored_approx_coeff, restored_detail_coeff \n",
    "\n",
    "def entropy_fn(signal):\n",
    "    entropy_val = 0\n",
    "    for i in signal:\n",
    "        entropy_val += (i**2)*(np.log2(i**2))     \n",
    "    return entropy_val\n",
    "\n",
    "def energy_fn(signal):\n",
    "    return np.sum(np.array(signal)**2)\n",
    "        \n",
    "import pywt\n",
    "def dwt_fn(signal):\n",
    "    #print(signal)\n",
    "    #coeffs = pywt.wavedec(signal, 'db4', level=4) \n",
    "    restored_approx_coeff,restored_detail_coeff = DWTfn(signal, 4, 'db4') \n",
    "    d4, d3, d2, d1 = restored_detail_coeff \n",
    "    \n",
    "#     print(len(d1))\n",
    "#     print(len(d2))\n",
    "#     print(len(d3))\n",
    "#     print(len(d4))\n",
    "#     raise Exception()\n",
    "    \n",
    "    bands = {'theta':d4,'alpha':d3,'beta':d2,'gamma':d1}\n",
    "    \n",
    "    band_instance = {}\n",
    "    \n",
    "    for band_name, band in bands.items():\n",
    "        band_instance[f\"{band_name}_entropy\"] = entropy_fn(band)\n",
    "        band_instance[f\"{band_name}_energy\"] = energy_fn(band)\n",
    "        band_instance[f\"{band_name}_mean\"] = np.mean(band)\n",
    "        band_instance[f\"{band_name}_std\"] = np.std(band)\n",
    "    \n",
    "    return band_instance\n",
    "\n",
    "from scipy.stats import kurtosis, skew, entropy\n",
    "def extract_time_domain_features(signal, verbose=False):\n",
    "    mean = np.mean(signal)\n",
    "    std = np.std(signal)\n",
    "    rnge = np.max(signal) - np.min(signal)\n",
    "    skewness = skew(signal)\n",
    "    kurt = kurtosis(signal)\n",
    "    hjorth_param_activity = std**2\n",
    "    hjorth_param_mobility, hjorth_param_complexity = pyeeg.hjorth(signal)    \n",
    "    #feature_vector = (mean,std,rnge,skewness,kurt,hjorth_param_activity,hjorth_param_mobility,hjorth_param_complexity)    \n",
    "    feature_vector_dict = {\"mean\":mean,\"std\":std,\"range\":rnge,\"skewness\":skewness,\"kurtosis\":kurt,\"hjorth_param_activity\":hjorth_param_activity, \"hjorth_param_mobility\":hjorth_param_mobility, \"hjorth_param_complexity\":hjorth_param_complexity}\n",
    "    \n",
    "    if verbose : print(feature_vector_dict)\n",
    "    return feature_vector_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-algebra",
   "metadata": {},
   "source": [
    "# DWT Feature Extraction\n",
    "\n",
    "5-level DWT \n",
    "Time (stats) and Time-Frequency (wavelet energy/relative/entropy)\n",
    "Dataset: 120 x 281160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "arranged-cooperative",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def feature_extraction(subjects, channel=[1,7,15,17,25], window_size=640, step_size=320, sample_rate=128, timedomain=True, timefreq=True, baseline=False, directory='data_python'):\n",
    "    usename=False\n",
    "    chan_title=str(len(channel))+\"chan\"\n",
    "    usename1=False\n",
    "    usename2=False\n",
    "    if channel == [1,7,15,17,25]:\n",
    "        usename1=True\n",
    "    if channel == [0,1,2,3,4]:\n",
    "        usename2=True\n",
    "    meta = []\n",
    "    \n",
    "    from os import path\n",
    "    if baseline: feature = \"dwt_baseline\"\n",
    "    else: feature = \"dwt\"\n",
    "        \n",
    "    tag_name = \"\"\n",
    "    extension = \"dat\"\n",
    "    if directory != \"data_python\":\n",
    "        tag_name = \"custom\"\n",
    "        extension = \"mat\"        \n",
    "    \n",
    "    if timedomain and timefreq: csv_filename = f'data{tag_name}/{feature}/{chan_title}_time_timefreq_{int(window_size/128)}s-{step_size/128}step.csv'\n",
    "    if timedomain and not timefreq: csv_filename = f'data{tag_name}/{feature}/{chan_title}_time_{int(window_size/128)}s-{step_size/128}step.csv'\n",
    "    if not timedomain and timefreq: csv_filename = f'data{tag_name}/{feature}/{chan_title}_timefreq_{int(window_size/128)}s-{step_size/128}step.csv'\n",
    "    print(csv_filename)\n",
    "    from os import path\n",
    "    if path.exists(csv_filename):\n",
    "        print(f\"{csv_filename} already exists.\")\n",
    "        return {\"csv_path\":csv_filename, \"data\":None}\n",
    "    \n",
    "    reuse_date_optimization = False\n",
    "    if feature == \"dwt_baseline\":\n",
    "        csv_filename_without = csv_filename.replace(\"dwt_baseline\",\"dwt\")\n",
    "        if path.exists(csv_filename_without):\n",
    "            print(f\"{csv_filename_without} already exists. Will use as trial data.\")\n",
    "            reuse_date_optimization = True\n",
    "            data_without = pd.read_csv(csv_filename_without)\n",
    "    \n",
    "    for sub in subjects:\n",
    "        #print(f\"Loading subject {sub}\")\n",
    "        subject_time = time.time()\n",
    "        try:\n",
    "            with open(f'../{directory}/s{sub}.{extension}', 'rb') as file:\n",
    "                subject = pickle.load(file, encoding='latin1') #resolve the python 2 data problem by encoding : latin1\n",
    "        except: \n",
    "            subject = sio.loadmat(f\"../{directory}/s{sub}.{extension}\")\n",
    "            \n",
    "            num_trials = len(subject[\"data\"])\n",
    "            for trial in range (0,num_trials):\n",
    "                eeg = subject[\"data\"][trial]\n",
    "\n",
    "                val = 1 if subject[\"labels\"][trial][0] >= 5 else 0\n",
    "                aro = 1 if subject[\"labels\"][trial][1] >= 5 else 0\n",
    "\n",
    "                if val == 0 and aro == 0:\n",
    "                    emotion = 0 #LALV\n",
    "                if val == 0 and aro == 1:\n",
    "                    emotion = 1 #HALV\n",
    "                if val == 1 and aro == 0:\n",
    "                    emotion = 2 #LAHV\n",
    "                if val == 1 and aro == 1:\n",
    "                    emotion = 3 #HAHV \n",
    "                \n",
    "                three_sec = 128*3\n",
    "                if baseline:\n",
    "                    baseline_instance = {\"Sub\":sub, \"Trial\":trial, \"Valence\":val, \"Arousal\":aro, \"Emotion\":emotion}\n",
    "                    for chan in channel:\n",
    "                        if usename1:\n",
    "                            if chan == 1: chan_name = \"AF3\"\n",
    "                            elif chan == 7: chan_name = \"T7\"\n",
    "                            elif chan == 15: chan_name = \"Pz\"\n",
    "                            elif chan == 17: chan_name = \"AF4\"\n",
    "                            elif chan == 25: chan_name =  \"T8\"\n",
    "                        if usename2:\n",
    "                            if chan == 0: chan_name = \"AF3\"\n",
    "                            elif chan == 1: chan_name = \"T7\"\n",
    "                            elif chan == 2: chan_name = \"Pz\"\n",
    "                            elif chan == 3: chan_name = \"AF4\"\n",
    "                            elif chan == 4: chan_name =  \"T8\"\n",
    "                        baseline_slice = eeg[chan][0 : three_sec]\n",
    "                                               \n",
    "                        if window_size == 384:\n",
    "                            #time domain                        \n",
    "                            if timedomain:\n",
    "                                time_domain_features = extract_time_domain_features(baseline_slice)\n",
    "                                for feature_name,value in time_domain_features.items():\n",
    "                                    if usename: baseline_instance[f\"{chan_name}_{feature_name}\"] = value\n",
    "                                    else: baseline_instance[f\"{chan}_{feature_name}\"] = value\n",
    "                            #time-frequency domain                            \n",
    "                            if timefreq:\n",
    "                                time_freq_feats = dwt_fn(baseline_slice)  \n",
    "                                for key,value in time_freq_feats.items():\n",
    "                                    if usename: baseline_instance[f\"{chan_name}_{key}\"] = value\n",
    "                                    else: baseline_instance[f\"{chan}_{key}\"] = value\n",
    "                                        \n",
    "                        elif window_size == 128:\n",
    "                            slices = [[0,128],[128,256],[256,384]]                                \n",
    "                            for time_slice in slices:\n",
    "                                baseline_mini_slice = baseline_slice[time_slice[0]:time_slice[1]]\n",
    "                                if timedomain:\n",
    "                                    time_domain_features = extract_time_domain_features(baseline_mini_slice)\n",
    "                                    for feature_name,value in time_domain_features.items():\n",
    "                                        if usename: \n",
    "                                            try: baseline_instance[f\"{chan_name}_{feature_name}\"] += value\n",
    "                                            except: baseline_instance[f\"{chan_name}_{feature_name}\"] = value\n",
    "                                        else: \n",
    "                                            try: baseline_instance[f\"{chan}_{feature_name}\"] += value\n",
    "                                            except: baseline_instance[f\"{chan}_{feature_name}\"] = value\n",
    "                                #time-frequency domain   \n",
    "                                if timefreq:\n",
    "                                    time_freq_feats = dwt_fn(baseline_mini_slice)  \n",
    "                                    for key,value in time_freq_feats.items():                                        \n",
    "                                        if usename: \n",
    "                                            try: baseline_instance[f\"{chan_name}_{key}\"] += value\n",
    "                                            except: baseline_instance[f\"{chan_name}_{key}\"] = value\n",
    "                                        else: \n",
    "                                            try: baseline_instance[f\"{chan}_{key}\"] += value\n",
    "                                            except: baseline_instance[f\"{chan}_{key}\"] = value    \n",
    "                                            \n",
    "                                                              \n",
    "                        else: raise Exception(\"Window size must be either 1 or 3 seconds long to use baseline.\")\n",
    "                    if window_size == 128:\n",
    "                        info_keys = [\"Sub\",\"Trial\", \"Valence\", \"Arousal\", \"Emotion\"] \n",
    "                        for key, value_bl in baseline_instance.items():\n",
    "                            if key not in info_keys:\n",
    "                                baseline_instance[key] = value_bl/3\n",
    "                if not reuse_date_optimization:\n",
    "                    start = three_sec\n",
    "                    while start + window_size < eeg.shape[1]:\n",
    "                        instance = {\"Sub\":sub, \"Trial\":trial, \"Valence\":val, \"Arousal\":aro, \"Emotion\":emotion}\n",
    "                        for chan in channel:                        \n",
    "                            eeg_slice = eeg[chan][start : start + window_size] \n",
    "                            eeg_standardized = stats.zscore(eeg_slice)                        \n",
    "\n",
    "                            if usename1:\n",
    "                                if chan == 1: chan_name = \"AF3\"\n",
    "                                elif chan == 7: chan_name = \"T7\"\n",
    "                                elif chan == 15: chan_name = \"Pz\"\n",
    "                                elif chan == 17: chan_name = \"AF4\"\n",
    "                                elif chan == 25: chan_name =  \"T8\"\n",
    "                            if usename2:\n",
    "                                if chan == 0: chan_name = \"AF3\"\n",
    "                                elif chan == 1: chan_name = \"T7\"\n",
    "                                elif chan == 2: chan_name = \"Pz\"\n",
    "                                elif chan == 3: chan_name = \"AF4\"\n",
    "                                elif chan == 4: chan_name =  \"T8\"\n",
    "\n",
    "                            #time domain\n",
    "\n",
    "                            if timedomain:\n",
    "                                time_domain_features = extract_time_domain_features(eeg_slice)\n",
    "                                for feature_name,value in time_domain_features.items():\n",
    "                                    if usename: instance[f\"{chan_name}_{feature_name}\"] = value\n",
    "                                    else: instance[f\"{chan}_{feature_name}\"] = value\n",
    "\n",
    "                            #time-frequency domain    \n",
    "\n",
    "                            if timefreq:\n",
    "                                time_freq_feats = dwt_fn(eeg_slice)  \n",
    "                                for key,value in time_freq_feats.items():\n",
    "                                    if usename: instance[f\"{chan_name}_{key}\"] = value\n",
    "                                    else: instance[f\"{chan}_{key}\"] = value   \n",
    "\n",
    "                        if baseline:\n",
    "                            #print(baseline_instance)\n",
    "                            info_keys = [\"Sub\",\"Trial\", \"Valence\", \"Arousal\", \"Emotion\"]\n",
    "                            for key,value_from_key_ffs in instance.items():\n",
    "                                if key not in info_keys:\n",
    "                                    #print(value_from_key_ffs)\n",
    "                                    #print(baseline_instance[key])\n",
    "                                    instance[key] = value_from_key_ffs - baseline_instance[key]\n",
    "                                    #print(instance[key],\"\\n\")\n",
    "                        meta.append(instance)    \n",
    "                        start = start + step_size\n",
    "                else:\n",
    "                    subject_num = int(sub.replace('0',''))\n",
    "                    data_to_use = data_without.loc[(data_without['Sub']==subject_num) & (data_without['Trial']==trial)].drop([\"Sub\", \"Trial\", \"Valence\", \"Arousal\", \"Emotion\"],axis=1)                   \n",
    "                    #baseline subtraction\n",
    "                    data_to_use = data_to_use.to_dict(orient='records')\n",
    "                    for row in data_to_use:\n",
    "                        instance = {\"Sub\":sub, \"Trial\":trial, \"Valence\":val, \"Arousal\":aro, \"Emotion\":emotion}\n",
    "                        for key,value in row.items():\n",
    "                            instance[key] = value - baseline_instance[key]\n",
    "                        meta.append(instance)  \n",
    "        print(f\"Completed subject {sub} in {round(time.time()-subject_time,2)}s\")\n",
    "        \n",
    "    df = pd.DataFrame(meta)   \n",
    "    \n",
    "    df.to_csv(csv_filename,index=False)\n",
    "        \n",
    "    return {\"csv_path\":csv_filename, \"data\":df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "intensive-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "def find_best_params(x, y, random_state=1):\n",
    "    #take smaller sample of full dataset, retaining class distribution    \n",
    "    \n",
    "    skf = StratifiedKFold(n_splits = 3, shuffle=True, random_state = random_state)        \n",
    "    for train_index, test_index in skf.split(x,y):\n",
    "        index_to_keep = test_index\n",
    "        break   \n",
    "    \n",
    "    x = x[x.index.isin(index_to_keep)].reset_index(drop=True)\n",
    "    y = y[y.index.isin(index_to_keep)].reset_index(drop=True)\n",
    "    \n",
    "    #lda = LDA(n_components=1)\n",
    "    #x = lda.fit_transform(x, y)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits = 3, shuffle=True, random_state = random_state) \n",
    "    param_grid = {'C':[1, 50, 100, 200, 300],'gamma':[0.00001,0.001,1, 50, 100], 'kernel':['rbf']}  \n",
    "    grid = GridSearchCV(svc_sklearn(), param_grid, refit = True, verbose=3, cv=skf.split(x,y), n_jobs=-1, scoring = 'accuracy')\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    x = scaler.fit_transform(x) \n",
    "    grid.fit(x, y)\n",
    "    \n",
    "    best_parameters = grid.best_params_\n",
    "    \n",
    "    return best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "scenic-riding",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "def subj_indept_target(target = None, df = None, hyperparameters=None, random_state=1, verbose=False):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    if target == \"val\" or target == \"valaro\": y = df.Valence\n",
    "    if target == \"aro\" or target == \"aroval\": y = df.Arousal\n",
    "\n",
    "    if target == \"val\" or target == \"aro\": x = df.drop(['Sub','Emotion','Trial','Valence','Arousal'],axis=1)\n",
    "    if target == \"valaro\": x = df.drop(['Sub','Emotion','Trial','Valence'],axis=1)\n",
    "    if target == \"aroval\": x = df.drop(['Sub','Emotion','Trial','Arousal'],axis=1)\n",
    "        \n",
    "#     skf = StratifiedKFold(n_splits = 8, shuffle=True, random_state = random_state)        \n",
    "#     for train_index, test_index in skf.split(x,y):\n",
    "#         index_to_keep = test_index\n",
    "#         break\n",
    "    \n",
    "#     hold_out_test_x = x[x.index.isin(index_to_keep)]\n",
    "#     hold_out_test_y = y[y.index.isin(index_to_keep)]\n",
    "    \n",
    "#     x = x[~x.index.isin(index_to_keep)].reset_index(drop=True)\n",
    "#     y = y[~y.index.isin(index_to_keep)].reset_index(drop=True)\n",
    "    \n",
    "    if not hyperparameters: params = find_best_params(x,y)\n",
    "    else: params = hyperparameters\n",
    "    \n",
    "#     ##################TEMP#########################\n",
    "#     print(params)\n",
    "#     print(time.time()-start_time)\n",
    "#     return None, None, None\n",
    "    \n",
    "#     ##################TEMP#########################\n",
    "    \n",
    "    \n",
    "    skf = StratifiedKFold(n_splits = 6, shuffle=True, random_state = random_state)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    x = scaler.fit_transform(x) \n",
    "\n",
    "    svm = svc_thundersvm()\n",
    "    svm.set_params(**params)\n",
    "    print(svm)\n",
    "\n",
    "    acc = []\n",
    "    for train_index, test_index in skf.split(x,y):\n",
    "        start_time = time.time()\n",
    "        x_train_fold, x_test_fold = x[train_index], x[test_index] \n",
    "        y_train_fold, y_test_fold = y[train_index], y[test_index] \n",
    "        \n",
    "        #lda = LDA(n_components=1)\n",
    "        #x_train_fold = lda.fit_transform(x_train_fold, y_train_fold)\n",
    "        \n",
    "        svm.fit(x_train_fold, y_train_fold) \n",
    "        \n",
    "        #x_test_fold = lda.transform(x_test_fold)\n",
    "\n",
    "        score_fold = svm.score(x_test_fold, y_test_fold)\n",
    "        acc.append(score_fold)\n",
    "        print(f\"Fold completed in {time.time()-start_time}\")\n",
    "    \n",
    "    acc = np.mean(acc)\n",
    "    std = np.std(acc)\n",
    "    \n",
    "    print(f\"{target}: {acc}±{std}\")\n",
    "    return (acc, params, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "minor-astronomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from thundersvm import SVC as svc_thundersvm\n",
    "from sklearn.svm import SVC as svc_sklearn\n",
    "from sklearn.model_selection import StratifiedKFold, ParameterGrid\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "def convert_dict_to_string(dictionary):\n",
    "    hyperstring = \"\"\n",
    "    for key,value in dictionary.items():\n",
    "        hyperstring+= f\"{key}:{value}, \"\n",
    "    hyperstring = hyperstring[:-2]\n",
    "    return hyperstring\n",
    "\n",
    "def subj_indept(csv_path=None, dataframe=None, verbose=False, hyperparameters=None, results_csv=\"results/Results - Indept - CUSTOM.csv\", to_save=True, baseline=False, custom=False):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except: \n",
    "        df = dataframe\n",
    "    val, val_params,valstd = subj_indept_target(target = \"val\", df = df, hyperparameters=hyperparameters)\n",
    "    aro, aro_params,arostd = subj_indept_target(target = \"aro\", df = df, hyperparameters=hyperparameters)\n",
    "    valaro, valaro_params,valarostd = subj_indept_target(target = \"valaro\", df = df, hyperparameters=hyperparameters)\n",
    "    aroval, aroval_params,arovalstd = subj_indept_target(target = \"aroval\", df = df, hyperparameters=hyperparameters)\n",
    "    \n",
    "    end_time = round(time.time()-start_time,2)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"VAL: {val}\")\n",
    "        print(f\"ARO: {aro}\")\n",
    "        print(f\"VALARO: {valaro}\")\n",
    "        print(f\"AROVAL: {aroval}\")\n",
    "        \n",
    "        print(f\"VAL x ARO: {val * aro}\")\n",
    "        print(f\"VAL x AROVAL: {val * aroval}\")\n",
    "        print(f\"ARO x VALARO: {aro * valaro}\")\n",
    "        \n",
    "        combo_dicts = {\"VALxARO\":val * aro, \"VALxAROVAL\":val * aroval, \"AROxVALARO\":aro * valaro}\n",
    "        combo_dicts = dict(sorted(combo_dicts.items(), key=lambda item: item[1], reverse=True))    \n",
    "\n",
    "        best_combination = list(combo_dicts.keys())[0]\n",
    "        best_acc = list(combo_dicts.values())[0]\n",
    "        \n",
    "        print(best_combination, best_acc)\n",
    "        print(f\"Completed in {end_time}s\")\n",
    "    if to_save:\n",
    "        results = pd.read_csv(results_csv)\n",
    "    \n",
    "        dependency = \"Independent\"\n",
    "        model_name = \"SVM\"\n",
    "        if baseline: model_name += \"-Baseline\"\n",
    "        if custom: model_name += \"-CUSTOM\"\n",
    "\n",
    "        if 'time_timefreq' in csv_path:\n",
    "            domain = \"T-TF\"\n",
    "        elif 'timefreq_' in csv_path and 'time_timefreq_' not in csv_path:\n",
    "            domain = \"TF\"\n",
    "        elif 'time_' in csv_path and 'time_timefreq_' not in csv_path:\n",
    "            domain = \"T\"\n",
    "\n",
    "        channels = int(str(str(csv_path.split(\"/\")[-1]).split(\"_\")[0]).split(\"chan\")[0])\n",
    "\n",
    "        window_size = int(str(str(str(csv_path.split(\"/\")[-1]).split(\"_\")[-1]).split(\"-\")[0]).split(\"s\")[0])\n",
    "        step_size = str(str(str(csv_path.split(\"/\")[-1]).split(\"_\")[-1]).split(\"-\")[1]).split(\"step\")[0]\n",
    "\n",
    "        combo_dicts = {\"VALxARO\":val * aro, \"VALxAROVAL\":val * aroval, \"AROxVALARO\":aro * valaro}\n",
    "        combo_dicts = dict(sorted(combo_dicts.items(), key=lambda item: item[1], reverse=True))    \n",
    "\n",
    "        best_combination = list(combo_dicts.keys())[0]\n",
    "        best_acc = list(combo_dicts.values())[0]\n",
    "\n",
    "        val_params = convert_dict_to_string(val_params)\n",
    "        aro_params = convert_dict_to_string(aro_params)\n",
    "        valaro_params = convert_dict_to_string(valaro_params)\n",
    "        aroval_params = convert_dict_to_string(aroval_params)\n",
    "\n",
    "        time_taken = end_time\n",
    "\n",
    "        val = round(val*100,2)\n",
    "        aro = round(aro*100,2)\n",
    "        valaro = round(valaro*100,2)\n",
    "        aroval = round(aroval*100,2)\n",
    "\n",
    "\n",
    "        new_result = {\"Dependency\":dependency, \"Model Name\":model_name, \"Domain\":domain, \"Channels\":channels, \n",
    "                      \"Window Size\":window_size, \"Step Size\": step_size, \"VAL\":f\"{val}\", \"ARO\":f\"{aro}\", \n",
    "                      \"VALARO\":f\"{valaro}\", \"AROVAL\":f\"{aroval}\", \"Best Combination\":best_combination, \"Best Acc\":best_acc, \n",
    "                      \"HP_VAL\":val_params, \"HP_ARO\":aro_params, \"HP_VALARO\":valaro_params, \"HP_AROVAL\":aroval_params, \n",
    "                      \"Time taken\":time_taken}\n",
    "\n",
    "        results = results.append(new_result, ignore_index=True)\n",
    "        results = results[[\"Dependency\",\"Model Name\",\"Domain\",\"Channels\", \"Window Size\",\"Step Size\",\n",
    "                           \"VAL\",\"ARO\",\"VALARO\",\"AROVAL\",\"Best Combination\",\"Best Acc\", \n",
    "                           \"HP_VAL\", \"HP_ARO\", \"HP_VALARO\", \"HP_AROVAL\", \"Time taken\"]]  \n",
    "\n",
    "        results.to_csv(results_csv, index=False)\n",
    "        print(f\"Completed. - {dependency}, {domain}, {channels}, {window_size}-{step_size} in {time_taken}s\")\n",
    "        print(f\"VAL:{val}±{val}\\nARO:{aro}±{arostd}\\nVALARO:{valaro}±{valarostd}\\nAROVAL:{aroval}±{arovalstd}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-charity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_rate = 128 \n",
    "subject_list = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','32']\n",
    "channels = [1,7,15,17,25]\n",
    "timefreq = True\n",
    "timedomain = False\n",
    "window_in_sec = 3 #[2,4,6,8]:\n",
    "baseline = True\n",
    "window_size = window_in_sec * sample_rate\n",
    "step_size = window_size\n",
    "                \n",
    "start_time = time.time()\n",
    "data = feature_extraction(subjects=subject_list, channel=channels, window_size=window_size, step_size=step_size, timedomain=timedomain, timefreq=timefreq, baseline=baseline)\n",
    "print(f\"Time taken to process dataset: {round(time.time()-start_time,2)}s.\")\n",
    "\n",
    "subj_indept(csv_path=data['csv_path'], verbose=False, baseline=baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "veterinary-amateur",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datacustom/dwt/5chan_timefreq_1s-1.0step.csv\n",
      "datacustom/dwt/5chan_timefreq_1s-1.0step.csv already exists.\n",
      "Time taken to process dataset: 0.0s.\n",
      "SVC(C=1, gamma=1)\n",
      "Fold completed in 12.223158597946167\n",
      "Fold completed in 12.417818546295166\n",
      "Fold completed in 12.548962116241455\n",
      "Fold completed in 12.566531896591187\n",
      "Fold completed in 12.12250566482544\n",
      "Fold completed in 12.820863008499146\n",
      "val: 0.6335824782750281±0.0\n",
      "SVC(C=1, gamma=1)\n",
      "Fold completed in 12.569934129714966\n",
      "Fold completed in 12.475698471069336\n",
      "Fold completed in 12.782794952392578\n",
      "Fold completed in 12.500403642654419\n",
      "Fold completed in 12.942951440811157\n",
      "Fold completed in 12.77852201461792\n",
      "aro: 0.6434439621039162±0.0\n",
      "SVC(C=1, gamma=1)\n",
      "Fold completed in 12.23851490020752\n",
      "Fold completed in 12.021023035049438\n",
      "Fold completed in 11.923681497573853\n",
      "Fold completed in 11.91264295578003\n",
      "Fold completed in 12.108025550842285\n",
      "Fold completed in 11.970342874526978\n",
      "valaro: 0.6556016958486046±0.0\n",
      "SVC(C=1, gamma=1)\n",
      "Fold completed in 12.720830202102661\n",
      "Fold completed in 13.142842054367065\n",
      "Fold completed in 13.001594066619873\n",
      "Fold completed in 12.789501428604126\n",
      "Fold completed in 12.9333655834198\n",
      "Fold completed in 13.022800922393799\n",
      "aroval: 0.6686088354870013±0.0\n",
      "VAL: 0.6335824782750281\n",
      "ARO: 0.6434439621039162\n",
      "VALARO: 0.6556016958486046\n",
      "AROVAL: 0.6686088354870013\n",
      "VAL x ARO: 0.4076748201409025\n",
      "VAL x AROVAL: 0.42361884298443486\n",
      "ARO x VALARO: 0.42184295273887273\n",
      "VALxAROVAL 0.42361884298443486\n",
      "Completed in 301.64s\n",
      "Completed. - Independent, TF, 5, 1-1.0 in 301.64s\n",
      "VAL:63.36±63.36\n",
      "ARO:64.34±0.0\n",
      "VALARO:65.56±0.0\n",
      "AROVAL:66.86±0.0\n",
      "\n",
      "datacustom/dwt_baseline/5chan_timefreq_1s-1.0step.csv\n",
      "datacustom/dwt_baseline/5chan_timefreq_1s-1.0step.csv already exists.\n",
      "Time taken to process dataset: 0.0s.\n",
      "SVC(C=50, gamma=1)\n",
      "Fold completed in 49.851004123687744\n",
      "Fold completed in 50.012476205825806\n",
      "Fold completed in 50.35282063484192\n",
      "Fold completed in 52.68191695213318\n",
      "Fold completed in 51.65210199356079\n",
      "Fold completed in 50.5227837562561\n",
      "val: 0.7958403294609325±0.0\n",
      "SVC(C=50, gamma=1)\n",
      "Fold completed in 50.99175763130188\n",
      "Fold completed in 50.751882553100586\n",
      "Fold completed in 49.658915758132935\n",
      "Fold completed in 49.739375829696655\n",
      "Fold completed in 49.50324201583862\n",
      "Fold completed in 48.911752223968506\n",
      "aro: 0.8054099340875122±0.0\n",
      "SVC(C=50, gamma=1)\n",
      "Fold completed in 41.12755513191223\n",
      "Fold completed in 42.196682929992676\n",
      "Fold completed in 41.284565687179565\n",
      "Fold completed in 41.71070981025696\n",
      "Fold completed in 41.846288442611694\n",
      "Fold completed in 43.397193908691406\n",
      "valaro: 0.8168243566886625±0.0\n",
      "SVC(C=50, gamma=1)\n",
      "Fold completed in 41.21748328208923\n",
      "Fold completed in 40.630396604537964\n",
      "Fold completed in 41.39451217651367\n",
      "Fold completed in 41.80132722854614\n",
      "Fold completed in 42.3086473941803\n",
      "Fold completed in 40.47451424598694\n",
      "aroval: 0.8256772401810161±0.0\n",
      "VAL: 0.7958403294609325\n",
      "ARO: 0.8054099340875122\n",
      "VALARO: 0.8168243566886625\n",
      "AROVAL: 0.8256772401810161\n",
      "VAL x ARO: 0.6409777072953137\n",
      "VAL x AROVAL: 0.6571072468540533\n",
      "ARO x VALARO: 0.6578784512816902\n",
      "AROxVALARO 0.6578784512816902\n",
      "Completed in 1105.03s\n",
      "Completed. - Independent, TF, 5, 1-1.0 in 1105.03s\n",
      "VAL:79.58±79.58\n",
      "ARO:80.54±0.0\n",
      "VALARO:81.68±0.0\n",
      "AROVAL:82.57±0.0\n",
      "\n",
      "datacustom/dwt/5chan_timefreq_3s-3.0step.csv\n",
      "datacustom/dwt/5chan_timefreq_3s-3.0step.csv already exists.\n",
      "Time taken to process dataset: 0.0s.\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:   15.2s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:   49.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1, gamma=1)\n",
      "Fold completed in 2.5461699962615967\n",
      "Fold completed in 2.4543802738189697\n",
      "Fold completed in 2.5696070194244385\n",
      "Fold completed in 2.483809471130371\n",
      "Fold completed in 2.53094482421875\n",
      "Fold completed in 2.5392110347747803\n",
      "val: 0.6486419196734348±0.0\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:   48.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1, gamma=1)\n",
      "Fold completed in 2.6204307079315186\n",
      "Fold completed in 2.5723185539245605\n",
      "Fold completed in 2.4944629669189453\n",
      "Fold completed in 2.559711456298828\n",
      "Fold completed in 2.4872443675994873\n",
      "Fold completed in 2.488931655883789\n",
      "aro: 0.6596462431270698±0.0\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:   47.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1, gamma=1)\n",
      "Fold completed in 2.5098628997802734\n",
      "Fold completed in 2.4668147563934326\n",
      "Fold completed in 2.5609822273254395\n",
      "Fold completed in 2.449598550796509\n",
      "Fold completed in 2.4129414558410645\n",
      "Fold completed in 2.4480979442596436\n",
      "valaro: 0.6723816483517648±0.0\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:   48.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1, gamma=1)\n",
      "Fold completed in 2.575129985809326\n",
      "Fold completed in 2.512087821960449\n",
      "Fold completed in 2.581465005874634\n",
      "Fold completed in 2.5916285514831543\n",
      "Fold completed in 2.5717689990997314\n",
      "Fold completed in 2.5498955249786377\n",
      "aroval: 0.6841689943069146±0.0\n",
      "VAL: 0.6486419196734348\n",
      "ARO: 0.6596462431270698\n",
      "VALARO: 0.6723816483517648\n",
      "AROVAL: 0.6841689943069146\n",
      "VAL x ARO: 0.4278742054473118\n",
      "VAL x AROVAL: 0.44378068984828034\n",
      "ARO x VALARO: 0.4435340282828282\n",
      "VALxAROVAL 0.44378068984828034\n",
      "Completed in 271.21s\n",
      "Completed. - Independent, TF, 5, 3-3.0 in 271.21s\n",
      "VAL:64.86±64.86\n",
      "ARO:65.96±0.0\n",
      "VALARO:67.24±0.0\n",
      "AROVAL:68.42±0.0\n",
      "\n",
      "datacustom/dwt_baseline/5chan_timefreq_3s-3.0step.csv\n",
      "datacustom/dwt_baseline/5chan_timefreq_3s-3.0step.csv already exists.\n",
      "Time taken to process dataset: 0.0s.\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:   13.1s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:   47.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=50, gamma=1)\n",
      "Fold completed in 8.535311698913574\n",
      "Fold completed in 7.854887008666992\n",
      "Fold completed in 8.179536581039429\n",
      "Fold completed in 7.832632303237915\n",
      "Fold completed in 7.980997800827026\n",
      "Fold completed in 8.601992845535278\n",
      "val: 0.8070311514392063±0.0\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:   45.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=50, gamma=1)\n",
      "Fold completed in 7.436862945556641\n",
      "Fold completed in 7.235740900039673\n",
      "Fold completed in 7.485136032104492\n",
      "Fold completed in 7.595627784729004\n",
      "Fold completed in 7.479313611984253\n",
      "Fold completed in 7.514719724655151\n",
      "aro: 0.8140789146089364±0.0\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:   42.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=50, gamma=1)\n",
      "Fold completed in 6.140585899353027\n",
      "Fold completed in 5.97540545463562\n",
      "Fold completed in 6.200981140136719\n",
      "Fold completed in 5.965474367141724\n",
      "Fold completed in 5.919128894805908\n",
      "Fold completed in 5.846118450164795\n",
      "valaro: 0.8274735188872603±0.0\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:   12.3s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:   44.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=50, gamma=1)\n",
      "Fold completed in 6.03823447227478\n",
      "Fold completed in 6.072747707366943\n",
      "Fold completed in 6.271881103515625\n",
      "Fold completed in 6.0506751537323\n",
      "Fold completed in 6.062435150146484\n",
      "Fold completed in 5.966654300689697\n",
      "aroval: 0.8343569384637729±0.0\n",
      "VAL: 0.8070311514392063\n",
      "ARO: 0.8140789146089364\n",
      "VALARO: 0.8274735188872603\n",
      "AROVAL: 0.8343569384637729\n",
      "VAL x ARO: 0.6569870438192293\n",
      "VAL x AROVAL: 0.6733520407597097\n",
      "ARO x VALARO: 0.6736287441233781\n",
      "AROxVALARO 0.6736287441233781\n",
      "Completed in 367.74s\n",
      "Completed. - Independent, TF, 5, 3-3.0 in 367.74s\n",
      "VAL:80.7±80.7\n",
      "ARO:81.41±0.0\n",
      "VALARO:82.75±0.0\n",
      "AROVAL:83.44±0.0\n",
      "\n",
      "datacustom/dwt/5chan_timefreq_5s-5.0step.csv\n",
      "datacustom/dwt/5chan_timefreq_5s-5.0step.csv already exists.\n",
      "Time taken to process dataset: 0.0s.\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:   12.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1, gamma=1)\n",
      "Fold completed in 1.2430827617645264\n",
      "Fold completed in 1.224421501159668\n",
      "Fold completed in 1.2023611068725586\n",
      "Fold completed in 1.2240700721740723\n",
      "Fold completed in 1.194342851638794\n",
      "Fold completed in 1.224273443222046\n",
      "val: 0.6530216746658807±0.0\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:   12.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1, gamma=1)\n",
      "Fold completed in 1.2546775341033936\n",
      "Fold completed in 1.223121166229248\n",
      "Fold completed in 1.2090294361114502\n",
      "Fold completed in 1.2085647583007812\n",
      "Fold completed in 1.223858118057251\n",
      "Fold completed in 1.2181384563446045\n",
      "aro: 0.6629183384640901±0.0\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:   12.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1, gamma=1)\n",
      "Fold completed in 1.217094898223877\n",
      "Fold completed in 1.1931002140045166\n",
      "Fold completed in 1.2146635055541992\n",
      "Fold completed in 1.1827325820922852\n",
      "Fold completed in 1.2028868198394775\n",
      "Fold completed in 1.1944632530212402\n",
      "valaro: 0.6769415254234197±0.0\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:   12.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1, gamma=1)\n",
      "Fold completed in 1.2852039337158203\n",
      "Fold completed in 1.195277452468872\n",
      "Fold completed in 1.2140202522277832\n",
      "Fold completed in 1.229724645614624\n",
      "Fold completed in 1.230684757232666\n",
      "Fold completed in 1.2236902713775635\n",
      "aroval: 0.6917495254399567±0.0\n",
      "VAL: 0.6530216746658807\n",
      "ARO: 0.6629183384640901\n",
      "VALARO: 0.6769415254234197\n",
      "AROVAL: 0.6917495254399567\n",
      "VAL x ARO: 0.43290004355054323\n",
      "VAL x AROVAL: 0.45172743355212874\n",
      "ARO x VALARO: 0.44875695127104\n",
      "VALxAROVAL 0.45172743355212874\n",
      "Completed in 84.57s\n",
      "Completed. - Independent, TF, 5, 5-5.0 in 84.57s\n",
      "VAL:65.3±65.3\n",
      "ARO:66.29±0.0\n",
      "VALARO:67.69±0.0\n",
      "AROVAL:69.17±0.0\n",
      "\n",
      "datacustom/dwt/5chan_timefreq_7s-7.0step.csv\n",
      "datacustom/dwt/5chan_timefreq_7s-7.0step.csv already exists.\n",
      "Time taken to process dataset: 0.0s.\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:    6.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1, gamma=1)\n",
      "Fold completed in 0.8904564380645752\n",
      "Fold completed in 0.8322446346282959\n",
      "Fold completed in 0.8450183868408203\n",
      "Fold completed in 0.8591063022613525\n",
      "Fold completed in 0.8448514938354492\n",
      "Fold completed in 0.8562452793121338\n",
      "val: 0.6528019377338992±0.0\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:    6.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1, gamma=1)\n",
      "Fold completed in 0.8975980281829834\n",
      "Fold completed in 0.8547482490539551\n",
      "Fold completed in 0.8674540519714355\n",
      "Fold completed in 0.868049144744873\n",
      "Fold completed in 0.8551244735717773\n",
      "Fold completed in 0.8473021984100342\n",
      "aro: 0.6619023731536178±0.0\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:    6.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1, gamma=1)\n",
      "Fold completed in 0.8847026824951172\n",
      "Fold completed in 0.845505952835083\n",
      "Fold completed in 0.8266406059265137\n",
      "Fold completed in 0.8335590362548828\n",
      "Fold completed in 0.8314483165740967\n",
      "Fold completed in 0.8323071002960205\n",
      "valaro: 0.6761955411972007±0.0\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:    6.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1, gamma=1)\n",
      "Fold completed in 0.9024190902709961\n",
      "Fold completed in 0.8432130813598633\n",
      "Fold completed in 0.864459753036499\n",
      "Fold completed in 0.8597497940063477\n",
      "Fold completed in 0.8640072345733643\n",
      "Fold completed in 0.8579981327056885\n",
      "aroval: 0.6841232460216862±0.0\n",
      "VAL: 0.6528019377338992\n",
      "ARO: 0.6619023731536178\n",
      "VALARO: 0.6761955411972007\n",
      "AROVAL: 0.6841232460216862\n",
      "VAL x ARO: 0.43209115178534807\n",
      "VAL x AROVAL: 0.44659698065176173\n",
      "ARO x VALARO: 0.44757543343432205\n",
      "AROxVALARO 0.44757543343432205\n",
      "Completed in 49.1s\n",
      "Completed. - Independent, TF, 5, 7-7.0 in 49.1s\n",
      "VAL:65.28±65.28\n",
      "ARO:66.19±0.0\n",
      "VALARO:67.62±0.0\n",
      "AROVAL:68.41±0.0\n",
      "\n",
      "datacustom/dwt/5chan_timefreq_9s-9.0step.csv\n",
      "datacustom/dwt/5chan_timefreq_9s-9.0step.csv already exists.\n",
      "Time taken to process dataset: 0.0s.\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  64 out of  75 | elapsed:    2.9s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:    3.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1, gamma=1)\n",
      "Fold completed in 0.627655029296875\n",
      "Fold completed in 0.6191937923431396\n",
      "Fold completed in 0.6268396377563477\n",
      "Fold completed in 0.6183090209960938\n",
      "Fold completed in 0.6066441535949707\n",
      "Fold completed in 0.6151449680328369\n",
      "val: 0.6588358131036283±0.0\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  64 out of  75 | elapsed:    2.9s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:    3.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1, gamma=1)\n",
      "Fold completed in 0.6870689392089844\n",
      "Fold completed in 0.6320264339447021\n",
      "Fold completed in 0.6244521141052246\n",
      "Fold completed in 0.6267085075378418\n",
      "Fold completed in 0.6372458934783936\n",
      "Fold completed in 0.6182699203491211\n",
      "aro: 0.666536152440616±0.0\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  64 out of  75 | elapsed:    3.0s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:    3.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1, gamma=1)\n",
      "Fold completed in 1.162623405456543\n",
      "Fold completed in 0.6884472370147705\n",
      "Fold completed in 0.6396713256835938\n",
      "Fold completed in 0.622988224029541\n",
      "Fold completed in 0.6209323406219482\n",
      "Fold completed in 0.6255033016204834\n",
      "valaro: 0.6752806055860089±0.0\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  64 out of  75 | elapsed:    3.2s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:    3.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1, gamma=1)\n",
      "Fold completed in 0.699988842010498\n",
      "Fold completed in 0.6784048080444336\n",
      "Fold completed in 0.6935176849365234\n",
      "Fold completed in 0.677016019821167\n",
      "Fold completed in 0.6805086135864258\n",
      "Fold completed in 0.681157112121582\n",
      "aroval: 0.6842860871835031±0.0\n",
      "VAL: 0.6588358131036283\n",
      "ARO: 0.666536152440616\n",
      "VALARO: 0.6752806055860089\n",
      "AROVAL: 0.6842860871835031\n",
      "VAL x ARO: 0.4391378879561772\n",
      "VAL x AROVAL: 0.45083218064504355\n",
      "ARO x VALARO: 0.4500989366650675\n",
      "VALxAROVAL 0.45083218064504355\n",
      "Completed in 32.42s\n",
      "Completed. - Independent, TF, 5, 9-9.0 in 32.42s\n",
      "VAL:65.88±65.88\n",
      "ARO:66.65±0.0\n",
      "VALARO:67.53±0.0\n",
      "AROVAL:68.43±0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_rate = 128 \n",
    "subject_list = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','32']\n",
    "\n",
    "#for baseline in [True, False]:\n",
    "    #for channels in [[1,7,15,17,25], list(range(0,32))]:\n",
    "for channels in [[0,1,2,3,4]]:\n",
    "    for timefreq in [True]:\n",
    "        for timedomain in [False]:\n",
    "#             if timedomain == False and timefreq == False:\n",
    "#                 continue\n",
    "            for window_in_sec in [1,3,5,7,9]:\n",
    "                for baseline in [False, True]:\n",
    "                    if baseline == True and window_in_sec > 3: continue  \n",
    "                    window_size = window_in_sec * sample_rate\n",
    "                    step_size = window_size\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    data = feature_extraction(subjects=subject_list, channel=channels, window_size=window_size, step_size=step_size, timedomain=timedomain, timefreq=timefreq, baseline=baseline, directory='DEAP_5chan_custom_preproc')\n",
    "                    print(f\"Time taken to process dataset: {round(time.time()-start_time,2)}s.\")\n",
    "                    if baseline == True and window_in_sec == 1:\n",
    "                        subj_indept(csv_path=data['csv_path'], verbose=True, baseline=baseline, custom=True, hyperparameters={'C':50,'gamma':1})\n",
    "                    elif baseline == False and window_in_sec == 1:\n",
    "                        subj_indept(csv_path=data['csv_path'], verbose=True, baseline=baseline, custom=True, hyperparameters={'C':1,'gamma':1})\n",
    "                    else: \n",
    "                        subj_indept(csv_path=data['csv_path'], verbose=True, baseline=baseline, custom=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "alternate-ethiopia",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! git clone https://github.com/Xtra-Computing/thundersvm.git\n",
    "# ! cd thundersvm && mkdir build && cd build && cmake .. && make -j\n",
    "# ! python /content/thundersvm/python/setup.py install\n",
    "\n",
    "# #-- Building for: Visual Studio 16 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "irish-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.machinery import SourceFileLoader\n",
    "thundersvm = SourceFileLoader(\"thundersvm\", r\"thundersvm\\python\\thundersvm\\thundersvm.py\").load_module()\n",
    "from thundersvm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "catholic-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "equivalent-shadow",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pickle\n",
    "import os\n",
    "import time\n",
    "from pywt import wavedec\n",
    "import pyeeg\n",
    "import scipy.io as sio\n",
    "\n",
    "from pathlib import Path\n",
    "cwd = os.getcwd()\n",
    "parent = Path(cwd).parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fancy-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywt import Wavelet\n",
    "from math import floor, ceil\n",
    "from numpy import concatenate, flipud, zeros, convolve, array\n",
    "\n",
    "def padding_symmetric(signal, size=8):\n",
    "    '''\n",
    "    Applies a symmetric padding of the specified size to the input signal.\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : ndarray\n",
    "        The signal to be padded.\n",
    "    size : int, optional\n",
    "        The size of the padding which corresponds to the size of the filter. The default is 8.\n",
    "    Returns\n",
    "    -------\n",
    "    padded_signal : ndarray\n",
    "        Padded signal.\n",
    "    '''\n",
    "    \n",
    "    padded_signal = concatenate([flipud(signal[:size]), signal, flipud(signal[-size:])])\n",
    "    return padded_signal\n",
    "\n",
    "\n",
    "def restore_signal(signal, reconstruction_filter, real_len):\n",
    "    '''\n",
    "    Restores the signal to its original size using the reconstruction filter.\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : ndarray\n",
    "        The signal to be restored.\n",
    "    reconstruction_filter : list\n",
    "        The reconstruction filter to be used for restoring the signal.\n",
    "    real_len : int\n",
    "        Real length of the signal.\n",
    "    Returns\n",
    "    -------\n",
    "    restored_signal : ndarray\n",
    "        Restored signal of the specified length.\n",
    "    '''\n",
    "    restored_signal = zeros(2 * len(signal) + 1)\n",
    "    for i in range(len(signal)):\n",
    "        restored_signal[i*2+1] = signal[i]\n",
    "    restored_signal = convolve(restored_signal, reconstruction_filter)\n",
    "    restored_len = len(restored_signal)\n",
    "    exceed_len = (restored_len - real_len) / 2\n",
    "    restored_signal = restored_signal[int(floor(exceed_len)):(restored_len - int(ceil(exceed_len)))]\n",
    "    return restored_signal\n",
    "\n",
    "def DWTfn(signal, level=3, mother_wavelet='db4'):\n",
    "    '''\n",
    "    Applies a Discrete Wavelet Transform to the signal.\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : ndarray\n",
    "        The signal on which the DWT will be applied.\n",
    "    level : int, optional\n",
    "        The decomposition levels for the DWT. The default is 3.\n",
    "    mother_wavelet : str, optional\n",
    "        The mother wavelet that it is going to be used in the DWT. The default is \"db4\".\n",
    "    Returns\n",
    "    -------\n",
    "    restored_approx_coeff : list\n",
    "        Restored approximations coefficients.\n",
    "    restored_detail_coeff : list\n",
    "        Restored detail coefficients.\n",
    "    '''\n",
    "    if type(signal).__name__ != \"ndarray\" and type(signal) != list:\n",
    "        raise TypeError(f\"'signal' must be 'ndarray', received: '{type(signal).__name__}'\")\n",
    "    if type(signal) == list:\n",
    "        signal = array(signal)\n",
    "    if \"float\" not in signal.dtype.name and \"int\" not in signal.dtype.name:\n",
    "        raise TypeError(f\"All elements of 'signal' must be numbers\")\n",
    "           \n",
    "    if type(level) != int:\n",
    "        raise TypeError(f\"'level' must be 'int', received: '{type(level).__name__}'\")\n",
    "    if level < 1:\n",
    "        raise TypeError(f\"'level' must be greater than 0, received: {level}\")\n",
    "        \n",
    "    if mother_wavelet not in ['haar', 'db1', 'db2', 'db3', 'db4', 'db5', 'db6', 'db7', 'db8', 'db9', 'db10', 'db11', 'db12', 'db13', 'db14', 'db15', 'db16', 'db17', 'db18', 'db19', 'db20', 'db21', 'db22', 'db23', 'db24', 'db25', 'db26', 'db27', 'db28', 'db29', 'db30', 'db31', 'db32', 'db33', 'db34', 'db35', 'db36', 'db37', 'db38', 'sym2', 'sym3', 'sym4', 'sym5', 'sym6', 'sym7', 'sym8', 'sym9', 'sym10', 'sym11', 'sym12', 'sym13', 'sym14', 'sym15', 'sym16', 'sym17', 'sym18', 'sym19', 'sym20', 'coif1', 'coif2', 'coif3', 'coif4', 'coif5', 'coif6', 'coif7', 'coif8', 'coif9', 'coif10', 'coif11', 'coif12', 'coif13', 'coif14', 'coif15', 'coif16', 'coif17', 'bior1.1', 'bior1.3', 'bior1.5', 'bior2.2', 'bior2.4', 'bior2.6', 'bior2.8', 'bior3.1', 'bior3.3', 'bior3.5', 'bior3.7', 'bior3.9', 'bior4.4', 'bior5.5', 'bior6.8', 'rbio1.1', 'rbio1.3', 'rbio1.5', 'rbio2.2', 'rbio2.4', 'rbio2.6', 'rbio2.8', 'rbio3.1', 'rbio3.3', 'rbio3.5', 'rbio3.7', 'rbio3.9', 'rbio4.4', 'rbio5.5', 'rbio6.8', 'dmey', 'gaus1', 'gaus2', 'gaus3', 'gaus4', 'gaus5', 'gaus6', 'gaus7', 'gaus8', 'mexh', 'morl', 'cgau1', 'cgau2', 'cgau3', 'cgau4', 'cgau5', 'cgau6', 'cgau7', 'cgau8', 'shan', 'fbsp', 'cmor']:\n",
    "        raise TypeError(f\"Invalid 'mother_wavelet' must be 'haar', 'db1', 'db2', 'db3', 'db4', 'db5', 'db6', 'db7', 'db8', 'db9', 'db10', 'db11', 'db12', 'db13', 'db14', 'db15', 'db16', 'db17', 'db18', 'db19', 'db20', 'db21', 'db22', 'db23', 'db24', 'db25', 'db26', 'db27', 'db28', 'db29', 'db30', 'db31', 'db32', 'db33', 'db34', 'db35', 'db36', 'db37', 'db38', 'sym2', 'sym3', 'sym4', 'sym5', 'sym6', 'sym7', 'sym8', 'sym9', 'sym10', 'sym11', 'sym12', 'sym13', 'sym14', 'sym15', 'sym16', 'sym17', 'sym18', 'sym19', 'sym20', 'coif1', 'coif2', 'coif3', 'coif4', 'coif5', 'coif6', 'coif7', 'coif8', 'coif9', 'coif10', 'coif11', 'coif12', 'coif13', 'coif14', 'coif15', 'coif16', 'coif17', 'bior1.1', 'bior1.3', 'bior1.5', 'bior2.2', 'bior2.4', 'bior2.6', 'bior2.8', 'bior3.1', 'bior3.3', 'bior3.5', 'bior3.7', 'bior3.9', 'bior4.4', 'bior5.5', 'bior6.8', 'rbio1.1', 'rbio1.3', 'rbio1.5', 'rbio2.2', 'rbio2.4', 'rbio2.6', 'rbio2.8', 'rbio3.1', 'rbio3.3', 'rbio3.5', 'rbio3.7', 'rbio3.9', 'rbio4.4', 'rbio5.5', 'rbio6.8', 'dmey', 'gaus1', 'gaus2', 'gaus3', 'gaus4', 'gaus5', 'gaus6', 'gaus7', 'gaus8', 'mexh', 'morl', 'cgau1', 'cgau2', 'cgau3', 'cgau4', 'cgau5', 'cgau6', 'cgau7', 'cgau8', 'shan', 'fbsp', or 'cmor', received: '{mother_wavelet}'\")\n",
    "        \n",
    "    original_len = len(signal)\n",
    "    approx_coeff = []\n",
    "    detail_coeff = []\n",
    "    wavelet = pywt.Wavelet(mother_wavelet)\n",
    "    low_filter = wavelet.dec_lo\n",
    "    high_filter = wavelet.dec_hi\n",
    "    filter_size = len(low_filter)\n",
    "    try:\n",
    "        for _ in range(level):\n",
    "            padded_signal = padding_symmetric(signal, filter_size)\n",
    "            low_pass_filtered_signal = convolve(padded_signal, low_filter)[filter_size:(2*filter_size)+len(signal)-1] \n",
    "            low_pass_filtered_signal = low_pass_filtered_signal[1:len(low_pass_filtered_signal):2]\n",
    "            high_pass_filtered_signal = convolve(padded_signal, high_filter)[filter_size:filter_size+len(signal)+filter_size-1]\n",
    "            high_pass_filtered_signal = high_pass_filtered_signal[1:len(high_pass_filtered_signal):2]\n",
    "            approx_coeff.append(low_pass_filtered_signal)\n",
    "            detail_coeff.append(high_pass_filtered_signal)\n",
    "            signal = low_pass_filtered_signal\n",
    "    except:\n",
    "        raise\n",
    "    low_reconstruction_filter = wavelet.rec_lo\n",
    "    high_reconstruction_filter = wavelet.rec_hi\n",
    "    real_lengths = []\n",
    "    for i in range(level-2,-1,-1):\n",
    "        real_lengths.append(len(approx_coeff[i]))\n",
    "    real_lengths.append(original_len)\n",
    "    restored_approx_coeff = []\n",
    "    for i in range(level):\n",
    "        restored_signal = restore_signal(approx_coeff[i], low_reconstruction_filter, real_lengths[level-1-i])\n",
    "        for j in range(i):\n",
    "            restored_signal = restore_signal(restored_signal, low_reconstruction_filter, real_lengths[level-i+j])\n",
    "        restored_approx_coeff.append(restored_signal)\n",
    "    restored_detail_coeff = []\n",
    "    for i in range(level):\n",
    "        restored_signal = restore_signal(detail_coeff[i], high_reconstruction_filter, real_lengths[level-1-i])\n",
    "        for j in range(i):\n",
    "            restored_signal = restore_signal(restored_signal, high_reconstruction_filter, real_lengths[level-i+j])\n",
    "        restored_detail_coeff.append(restored_signal)\n",
    "    return restored_approx_coeff, restored_detail_coeff \n",
    "\n",
    "def entropy_fn(signal):\n",
    "    entropy_val = 0\n",
    "    for i in signal:\n",
    "        entropy_val += (i**2)*(np.log2(i**2))     \n",
    "    return entropy_val\n",
    "\n",
    "def energy_fn(signal):\n",
    "    return np.sum(np.array(signal)**2)\n",
    "        \n",
    "import pywt\n",
    "def dwt_fn(signal):\n",
    "    #print(signal)\n",
    "    #coeffs = pywt.wavedec(signal, 'db4', level=4) \n",
    "    restored_approx_coeff,restored_detail_coeff = DWTfn(signal, 4, 'db4') \n",
    "    d4, d3, d2, d1 = restored_detail_coeff \n",
    "    \n",
    "#     print(len(d1))\n",
    "#     print(len(d2))\n",
    "#     print(len(d3))\n",
    "#     print(len(d4))\n",
    "#     raise Exception()\n",
    "    \n",
    "    bands = {'theta':d4,'alpha':d3,'beta':d2,'gamma':d1}\n",
    "    \n",
    "    band_instance = {}\n",
    "    \n",
    "    for band_name, band in bands.items():\n",
    "        band_instance[f\"{band_name}_entropy\"] = entropy_fn(band)\n",
    "        band_instance[f\"{band_name}_energy\"] = energy_fn(band)\n",
    "    \n",
    "    return band_instance\n",
    "\n",
    "from scipy.stats import kurtosis, skew, entropy\n",
    "def extract_time_domain_features(signal, verbose=False):\n",
    "    mean = np.mean(signal)\n",
    "    std = np.std(signal)\n",
    "    rnge = np.max(signal) - np.min(signal)\n",
    "    skewness = skew(signal)\n",
    "    kurt = kurtosis(signal)\n",
    "    hjorth_param_activity = std**2\n",
    "    hjorth_param_mobility, hjorth_param_complexity = pyeeg.hjorth(signal)    \n",
    "    #feature_vector = (mean,std,rnge,skewness,kurt,hjorth_param_activity,hjorth_param_mobility,hjorth_param_complexity)    \n",
    "    feature_vector_dict = {\"mean\":mean,\"std\":std,\"range\":rnge,\"skewness\":skewness,\"kurtosis\":kurt,\"hjorth_param_activity\":hjorth_param_activity, \"hjorth_param_mobility\":hjorth_param_mobility, \"hjorth_param_complexity\":hjorth_param_complexity}\n",
    "    \n",
    "    if verbose : print(feature_vector_dict)\n",
    "    return feature_vector_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-algebra",
   "metadata": {},
   "source": [
    "# DWT Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "arranged-cooperative",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def feature_extraction(subjects, channel=[1,7,15,17,25], window_size=640, step_size=320, sample_rate=128, timedomain=True, timefreq=True, baseline=False, directory='data_python'):\n",
    "    usename=False\n",
    "    chan_title=str(len(channel))+\"chan\"\n",
    "    usename1=False\n",
    "    usename2=False\n",
    "    if channel == [1,7,15,17,25]:\n",
    "        usename1=True\n",
    "    if channel == [0,1,2,3,4]:\n",
    "        usename2=True\n",
    "    meta = []\n",
    "    \n",
    "    from os import path\n",
    "    if baseline: feature = \"dwt_baseline\"\n",
    "    else: feature = \"dwt\"\n",
    "        \n",
    "    tag_name = \"\"\n",
    "    extension = \"dat\"\n",
    "    if directory != \"data_python\":\n",
    "        tag_name = \"custom\"\n",
    "        extension = \"mat\"\n",
    "    \n",
    "    if timedomain and timefreq: csv_filename = f'data{tag_name}/{feature}/{chan_title}_time_timefreq_{int(window_size/128)}s-{step_size/128}step.csv'\n",
    "    if timedomain and not timefreq: csv_filename = f'data{tag_name}/{feature}/{chan_title}_time_{int(window_size/128)}s-{step_size/128}step.csv'\n",
    "    if not timedomain and timefreq: csv_filename = f'data{tag_name}/{feature}/{chan_title}_timefreq_{int(window_size/128)}s-{step_size/128}step.csv'\n",
    "    print(csv_filename)\n",
    "    from os import path\n",
    "    if path.exists(csv_filename):\n",
    "        print(f\"{csv_filename} already exists.\")\n",
    "        return {\"csv_path\":csv_filename, \"data\":None}\n",
    "    \n",
    "    reuse_date_optimization = False\n",
    "    if feature == \"dwt_baseline\":\n",
    "        csv_filename_without = csv_filename.replace(\"dwt_baseline\",\"dwt\")\n",
    "        if path.exists(csv_filename_without):\n",
    "            print(f\"{csv_filename_without} already exists. Will use as trial data.\")\n",
    "            reuse_date_optimization = True\n",
    "            data_without = pd.read_csv(csv_filename_without)\n",
    "    \n",
    "    for sub in subjects:\n",
    "        #print(f\"Loading subject {sub}\")\n",
    "        subject_time = time.time()\n",
    "        try:\n",
    "            with open(f'../{directory}/s{sub}.{extension}', 'rb') as file:\n",
    "                subject = pickle.load(file, encoding='latin1') #resolve the python 2 data problem by encoding : latin1\n",
    "        except: \n",
    "            subject = sio.loadmat(f\"../{directory}/s{sub}.{extension}\")\n",
    "            \n",
    "            num_trials = len(subject[\"data\"])\n",
    "            for trial in range (0,num_trials):\n",
    "                eeg = subject[\"data\"][trial]\n",
    "\n",
    "                val = 1 if subject[\"labels\"][trial][0] >= 5 else 0\n",
    "                aro = 1 if subject[\"labels\"][trial][1] >= 5 else 0\n",
    "\n",
    "                if val == 0 and aro == 0:\n",
    "                    emotion = 0 #LALV\n",
    "                if val == 0 and aro == 1:\n",
    "                    emotion = 1 #HALV\n",
    "                if val == 1 and aro == 0:\n",
    "                    emotion = 2 #LAHV\n",
    "                if val == 1 and aro == 1:\n",
    "                    emotion = 3 #HAHV \n",
    "                \n",
    "                three_sec = 128*3\n",
    "                if baseline:\n",
    "                    baseline_instance = {\"Sub\":sub, \"Trial\":trial, \"Valence\":val, \"Arousal\":aro, \"Emotion\":emotion}\n",
    "                    for chan in channel:\n",
    "                        if usename1:\n",
    "                            if chan == 1: chan_name = \"AF3\"\n",
    "                            elif chan == 7: chan_name = \"T7\"\n",
    "                            elif chan == 15: chan_name = \"Pz\"\n",
    "                            elif chan == 17: chan_name = \"AF4\"\n",
    "                            elif chan == 25: chan_name =  \"T8\"\n",
    "                        if usename2:\n",
    "                            if chan == 0: chan_name = \"AF3\"\n",
    "                            elif chan == 1: chan_name = \"T7\"\n",
    "                            elif chan == 2: chan_name = \"Pz\"\n",
    "                            elif chan == 3: chan_name = \"AF4\"\n",
    "                            elif chan == 4: chan_name =  \"T8\"\n",
    "                        baseline_slice = eeg[chan][0 : three_sec]\n",
    "                                               \n",
    "                        if window_size == 384:\n",
    "                            #time domain                        \n",
    "                            if timedomain:\n",
    "                                time_domain_features = extract_time_domain_features(baseline_slice)\n",
    "                                for feature_name,value in time_domain_features.items():\n",
    "                                    if usename: baseline_instance[f\"{chan_name}_{feature_name}\"] = value\n",
    "                                    else: baseline_instance[f\"{chan}_{feature_name}\"] = value\n",
    "                            #time-frequency domain                            \n",
    "                            if timefreq:\n",
    "                                time_freq_feats = dwt_fn(baseline_slice)  \n",
    "                                for key,value in time_freq_feats.items():\n",
    "                                    if usename: baseline_instance[f\"{chan_name}_{key}\"] = value\n",
    "                                    else: baseline_instance[f\"{chan}_{key}\"] = value\n",
    "                                        \n",
    "                        elif window_size == 128:\n",
    "                            slices = [[0,128],[128,256],[256,384]]                                \n",
    "                            for time_slice in slices:\n",
    "                                baseline_mini_slice = baseline_slice[time_slice[0]:time_slice[1]]\n",
    "                                if timedomain:\n",
    "                                    time_domain_features = extract_time_domain_features(baseline_mini_slice)\n",
    "                                    for feature_name,value in time_domain_features.items():\n",
    "                                        if usename: \n",
    "                                            try: baseline_instance[f\"{chan_name}_{feature_name}\"] += value\n",
    "                                            except: baseline_instance[f\"{chan_name}_{feature_name}\"] = value\n",
    "                                        else: \n",
    "                                            try: baseline_instance[f\"{chan}_{feature_name}\"] += value\n",
    "                                            except: baseline_instance[f\"{chan}_{feature_name}\"] = value\n",
    "                                #time-frequency domain   \n",
    "                                if timefreq:\n",
    "                                    time_freq_feats = dwt_fn(baseline_mini_slice)  \n",
    "                                    for key,value in time_freq_feats.items():                                        \n",
    "                                        if usename: \n",
    "                                            try: baseline_instance[f\"{chan_name}_{key}\"] += value\n",
    "                                            except: baseline_instance[f\"{chan_name}_{key}\"] = value\n",
    "                                        else: \n",
    "                                            try: baseline_instance[f\"{chan}_{key}\"] += value\n",
    "                                            except: baseline_instance[f\"{chan}_{key}\"] = value    \n",
    "                                            \n",
    "                                                              \n",
    "                        else: raise Exception(\"Window size must be either 1 or 3 seconds long to use baseline.\")\n",
    "                    if window_size == 128:\n",
    "                        info_keys = [\"Sub\",\"Trial\", \"Valence\", \"Arousal\", \"Emotion\"] \n",
    "                        for key, value_bl in baseline_instance.items():\n",
    "                            if key not in info_keys:\n",
    "                                baseline_instance[key] = value_bl/3\n",
    "                if not reuse_date_optimization:\n",
    "                    start = three_sec\n",
    "                    while start + window_size < eeg.shape[1]:\n",
    "                        instance = {\"Sub\":sub, \"Trial\":trial, \"Valence\":val, \"Arousal\":aro, \"Emotion\":emotion}\n",
    "                        for chan in channel:                        \n",
    "                            eeg_slice = eeg[chan][start : start + window_size] \n",
    "                            eeg_standardized = stats.zscore(eeg_slice)                        \n",
    "\n",
    "                            if usename1:\n",
    "                                if chan == 1: chan_name = \"AF3\"\n",
    "                                elif chan == 7: chan_name = \"T7\"\n",
    "                                elif chan == 15: chan_name = \"Pz\"\n",
    "                                elif chan == 17: chan_name = \"AF4\"\n",
    "                                elif chan == 25: chan_name =  \"T8\"\n",
    "                            if usename2:\n",
    "                                if chan == 0: chan_name = \"AF3\"\n",
    "                                elif chan == 1: chan_name = \"T7\"\n",
    "                                elif chan == 2: chan_name = \"Pz\"\n",
    "                                elif chan == 3: chan_name = \"AF4\"\n",
    "                                elif chan == 4: chan_name =  \"T8\"\n",
    "\n",
    "                            #time domain\n",
    "\n",
    "                            if timedomain:\n",
    "                                time_domain_features = extract_time_domain_features(eeg_slice)\n",
    "                                for feature_name,value in time_domain_features.items():\n",
    "                                    if usename: instance[f\"{chan_name}_{feature_name}\"] = value\n",
    "                                    else: instance[f\"{chan}_{feature_name}\"] = value\n",
    "\n",
    "                            #time-frequency domain    \n",
    "\n",
    "                            if timefreq:\n",
    "                                time_freq_feats = dwt_fn(eeg_slice)  \n",
    "                                for key,value in time_freq_feats.items():\n",
    "                                    if usename: instance[f\"{chan_name}_{key}\"] = value\n",
    "                                    else: instance[f\"{chan}_{key}\"] = value   \n",
    "\n",
    "                        if baseline:\n",
    "                            #print(baseline_instance)\n",
    "                            info_keys = [\"Sub\",\"Trial\", \"Valence\", \"Arousal\", \"Emotion\"]\n",
    "                            for key,value_from_key_ffs in instance.items():\n",
    "                                if key not in info_keys:\n",
    "                                    #print(value_from_key_ffs)\n",
    "                                    #print(baseline_instance[key])\n",
    "                                    instance[key] = value_from_key_ffs - baseline_instance[key]\n",
    "                                    #print(instance[key],\"\\n\")\n",
    "                        meta.append(instance)    \n",
    "                        start = start + step_size\n",
    "                else:\n",
    "                    subject_num = int(sub.replace('0',''))\n",
    "                    data_to_use = data_without.loc[(data_without['Sub']==subject_num) & (data_without['Trial']==trial)].drop([\"Sub\", \"Trial\", \"Valence\", \"Arousal\", \"Emotion\"],axis=1)                   \n",
    "                    #baseline subtraction\n",
    "                    data_to_use = data_to_use.to_dict(orient='records')\n",
    "                    for row in data_to_use:\n",
    "                        instance = {\"Sub\":sub, \"Trial\":trial, \"Valence\":val, \"Arousal\":aro, \"Emotion\":emotion}\n",
    "                        for key,value in row.items():\n",
    "                            instance[key] = value - baseline_instance[key]\n",
    "                        meta.append(instance)  \n",
    "        print(f\"Completed subject {sub} in {round(time.time()-subject_time,2)}s\")\n",
    "        \n",
    "    df = pd.DataFrame(meta)   \n",
    "    \n",
    "    df.to_csv(csv_filename,index=False)\n",
    "        \n",
    "    return {\"csv_path\":csv_filename, \"data\":df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "typical-protein",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sample_rate = 128 \n",
    "\n",
    "# subject_list = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','32']\n",
    "# #subject_list = ['01']\n",
    "# channels = list(range(0,32)) #32chan\n",
    "# #channels = [1,7,15,17,25] #5chan\n",
    "# # list(range(0,32)) 32chan\n",
    "# # [0,16,2,19]\n",
    "\n",
    "# window_in_sec = 5\n",
    "# window_size = window_in_sec*sample_rate\n",
    "# #step_size = int(window_size/2)\n",
    "# step_size = window_size\n",
    "\n",
    "# timedomain=False\n",
    "# timefreq=True\n",
    "\n",
    "# start_time = time.time()\n",
    "# data = feature_extraction(subjects=subject_list, channel=channels, window_size=window_size, step_size=step_size, timedomain=timedomain, timefreq=timefreq)\n",
    "# print(f\"Size of generated data: {data.shape}\")\n",
    "# print(f\"Time taken to process dataset: {round(time.time()-start_time,2)}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "numeric-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_params(target, df, random_state=1):\n",
    "    \n",
    "    if target == \"val\" or target == \"valaro\": y = df.Valence\n",
    "    if target == \"aro\" or target == \"aroval\": y = df.Arousal\n",
    "    if target == \"val\" or target == \"aro\": x = df.drop(['Sub','Emotion','Trial','Valence','Arousal'],axis=1)\n",
    "    if target == \"valaro\": x = df.drop(['Sub','Emotion','Trial','Valence'],axis=1)\n",
    "    if target == \"aroval\": x = df.drop(['Sub','Emotion','Trial','Arousal'],axis=1)\n",
    "        \n",
    "    skf = StratifiedKFold(n_splits = 3, shuffle=True, random_state = random_state)        \n",
    "    for train_index, test_index in skf.split(x,y):\n",
    "        index_to_keep = test_index\n",
    "        break\n",
    "    \n",
    "    hold_out_test_x = x[x.index.isin(index_to_keep)]\n",
    "    hold_out_test_y = y[y.index.isin(index_to_keep)]\n",
    "    \n",
    "    x = x[~x.index.isin(index_to_keep)].reset_index(drop=True)\n",
    "    y = y[~y.index.isin(index_to_keep)].reset_index(drop=True)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits = 7, shuffle=True, random_state = random_state)\n",
    "    param_grid = {'C':[1, 50, 100, 200, 300],'gamma':[0.00001,0.001,1, 50, 100], 'kernel':['rbf']}      \n",
    "    grid = GridSearchCV(SVC(), param_grid, refit = True, verbose=0, n_jobs=1, cv=skf.split(x,y), scoring = 'accuracy')\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    x = scaler.fit_transform(x) \n",
    "    grid.fit(x, y)\n",
    "    \n",
    "    best_parameters = grid.best_params_\n",
    "\n",
    "    return best_parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "numerous-regard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_to_string(dictionary):\n",
    "    hyperstring = \"\"\n",
    "    for key,value in dictionary.items():\n",
    "        hyperstring+= f\"{key}:{value}, \"\n",
    "    hyperstring = hyperstring[:-2]\n",
    "    return hyperstring\n",
    "\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "def subj_dept_target(target = None, df = None, hyperparameters=None, random_state=1, verbose=True):    \n",
    "    subject_dfs = []\n",
    "    for subject in range(1,33):\n",
    "        subject_dfs.append(df[df['Sub'].isin([subject])].reset_index(drop=True))\n",
    "    \n",
    "    #gridsearch on one subject to find best params \n",
    "    #params = find_best_params(target,subject_dfs[0])\n",
    "    \n",
    "    #iterate over all subjects    \n",
    "    full_start_time = time.time()\n",
    "    \n",
    "    acc = []\n",
    "    prec = []\n",
    "    rec = []\n",
    "    f1 = []\n",
    "    num = 1\n",
    "    print(f\"{target}\")\n",
    "    \n",
    "    param_subs = {}\n",
    "    \n",
    "    for df in subject_dfs:\n",
    "        start_time = time.time()\n",
    "        params = find_best_params(target,df)\n",
    "        \n",
    "        param_subs[f\"Subject{num}_{target}\"] = convert_dict_to_string(params)\n",
    "        #print(f\"sub {num}...\")\n",
    "        #subj_acc = leave_one_out(target=target, df=df, hyperparameters=hyperparameters, random_state=1, verbose=False)\n",
    "        #print(num, np.mean(subj_acc))\n",
    "        if target == \"val\" or target == \"valaro\": y = df.Valence\n",
    "        if target == \"aro\" or target == \"aroval\": y = df.Arousal\n",
    "\n",
    "        if target == \"val\" or target == \"aro\": x = df.drop(['Sub','Emotion','Trial','Valence','Arousal'],axis=1)\n",
    "        if target == \"valaro\": x = df.drop(['Sub','Emotion','Trial','Valence'],axis=1)\n",
    "        if target == \"aroval\": x = df.drop(['Sub','Emotion','Trial','Arousal'],axis=1)\n",
    "        scaler = StandardScaler()\n",
    "        x = scaler.fit_transform(x) \n",
    "        \n",
    "        svm = SVC()\n",
    "        svm.set_params(**params)\n",
    "        skf = StratifiedKFold(n_splits = 8, shuffle=True, random_state = random_state)        \n",
    "        \n",
    "        subj_acc = []\n",
    "        subj_prec = []\n",
    "        subj_rec = []\n",
    "        subj_f1 = []\n",
    "        \n",
    "        for train_index, test_index in skf.split(x,y):\n",
    "            x_train_fold, x_test_fold = x[train_index], x[test_index] \n",
    "            y_train_fold, y_test_fold = y[train_index], y[test_index] \n",
    "            svm.fit(x_train_fold, y_train_fold) \n",
    "\n",
    "            score_fold = svm.score(x_test_fold, y_test_fold)\n",
    "            subj_acc.append(score_fold)\n",
    "            \n",
    "            y_pred = svm.predict(x_test_fold)\n",
    "            #subj_prec.append(precision_score(y_test_fold, y_pred))\n",
    "            #subj_rec.append(recall_score(y_test_fold, y_pred))\n",
    "            #subj_f1.append(f1_score(y_test_fold, y_pred))\n",
    "            \n",
    "        acc.append(np.mean(subj_acc))\n",
    "#         prec.append(np.mean(subj_prec))\n",
    "#         rec.append(np.mean(subj_rec))\n",
    "#         f1.append(np.mean(subj_f1))        \n",
    "        \n",
    "#         dict_res = {'Accuracy': subj_acc, 'Precision': subj_prec, 'Recall':subj_rec, 'F1': subj_f1} \n",
    "#         df_res = pd.DataFrame(dict_res)        \n",
    "#         print(\"Subject\",num,\":\",target)\n",
    "#         print(df_res.describe())\n",
    "        num+=1\n",
    "        #print(f\"... took {round(time.time()-start_time,2)}s.\")\n",
    "        if num == 2: print(f\"[Estimated time for all subs: ~{round(time.time()-start_time,2) * 32}s]\")\n",
    "        \n",
    "    if verbose: print(f\"{target} completed - {time.time()-full_start_time}s\")\n",
    "    #return (np.mean(acc), params)\n",
    "    return (np.mean(acc), np.std(acc), acc, param_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "minor-astronomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold, ParameterGrid\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time    \n",
    "\n",
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "def get_params_from_indept(dependency, model_name, domain, channels, window_size, step_size):\n",
    "    results_indept = pd.read_csv('Results/Results - Indept - CUSTOM.csv')\n",
    "    \n",
    "    results_indept = results_indept.loc[results_indept['Domain'] == domain]\n",
    "    results_indept = results_indept.loc[results_indept['Window Size'] == window_size]\n",
    "    results_indept = results_indept.loc[results_indept['Channels'] == channels]\n",
    "    \n",
    "    try: params = [results_indept['HP_VAL'].iloc[0], results_indept['HP_ARO'].iloc[0], results_indept['HP_VALARO'].iloc[0], results_indept['HP_AROVAL'].iloc[0]]\n",
    "    except: params = ['C:200', 'C:200', 'C:200', 'C:200']\n",
    "    params_dicts = []\n",
    "    #params = ['C:200', 'C:200', 'C:200', 'C:200']\n",
    "    for param in params:\n",
    "        param = param.split(\", \")\n",
    "        current_dict = {}\n",
    "        for p in param:\n",
    "            p = p.split(\":\")\n",
    "            if p[1] == \"None\": current_dict[p[0]] = None\n",
    "            elif \".\" in p[1]: current_dict[p[0]] = float(p[1])\n",
    "            elif hasNumbers(p[1]): current_dict[p[0]] = int(p[1])\n",
    "            else: current_dict[p[0]] = p[1]            \n",
    "        params_dicts.append(current_dict)\n",
    "    \n",
    "    return params_dicts[0], params_dicts[1], params_dicts[2], params_dicts[3]\n",
    "\n",
    "def convert_dict_to_string(dictionary):\n",
    "    hyperstring = \"\"\n",
    "    for key,value in dictionary.items():\n",
    "        hyperstring+= f\"{key}:{value}, \"\n",
    "    hyperstring = hyperstring[:-2]\n",
    "    return hyperstring\n",
    "\n",
    "def subj_dept(csv_path=None, verbose=False, results_csv=\"results/Results - Dept - Subject - CUSTOM - Hyper.csv\", baseline=False, custom=False):\n",
    "    dependency = \"Dependent\"\n",
    "    model_name = \"SVM\"\n",
    "    \n",
    "    if 'time_timefreq' in csv_path:\n",
    "        domain = \"T-TF\"\n",
    "    elif 'timefreq_' in csv_path and 'time_timefreq_' not in csv_path:\n",
    "        domain = \"TF\"\n",
    "    elif 'time_' in csv_path and 'time_timefreq_' not in csv_path:\n",
    "        domain = \"T\"\n",
    "    \n",
    "    channels = int(str(str(csv_path.split(\"/\")[-1]).split(\"_\")[0]).split(\"chan\")[0])\n",
    "    \n",
    "    window_size = int(str(str(str(csv_path.split(\"/\")[-1]).split(\"_\")[-1]).split(\"-\")[0]).split(\"s\")[0])\n",
    "    step_size = str(str(str(csv_path.split(\"/\")[-1]).split(\"_\")[-1]).split(\"-\")[1]).split(\"step\")[0]\n",
    "    \n",
    "    #----------------------------------------------------------------\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #get hyperparams for each case from independent\n",
    "    val_params, aro_params, valaro_params, aroval_params = get_params_from_indept(dependency, model_name, domain, channels, window_size, step_size)    \n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    val,valstd,val_sub_list,val_param_subs = subj_dept_target(target = \"val\", df = df, hyperparameters=val_params)\n",
    "    aro,arostd,aro_sub_list,aro_param_subs = subj_dept_target(target = \"aro\", df = df, hyperparameters=aro_params)\n",
    "    valaro,valarostd,valaro_sub_list,valaro_param_subs = subj_dept_target(target = \"valaro\", df = df, hyperparameters=valaro_params)\n",
    "    aroval,arovalstd,aroval_sub_list,aroval_param_subs = subj_dept_target(target = \"aroval\", df = df, hyperparameters=aroval_params)\n",
    "    \n",
    "    end_time = round(time.time()-start_time,2)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"VAL: {val}\")\n",
    "        print(f\"ARO: {aro}\")\n",
    "        print(f\"VALARO: {valaro}\")\n",
    "        print(f\"AROVAL: {aroval}\")\n",
    "        \n",
    "        print(f\"VAL x ARO: {val * aro}\")\n",
    "        print(f\"VAL x AROVAL: {val * aroval}\")\n",
    "        print(f\"ARO x VALARO: {aro * valaro}\")\n",
    "    \n",
    "    results = pd.read_csv(results_csv)\n",
    "    \n",
    "    dependency = \"Dependent\"\n",
    "    model_name = \"SVM\"\n",
    "    if baseline: model_name += \"-Baseline\"\n",
    "    if custom: model_name += \"-CUSTOM\"\n",
    "    \n",
    "    if 'time_timefreq' in csv_path:\n",
    "        domain = \"T-TF\"\n",
    "    elif 'timefreq_' in csv_path and 'time_timefreq_' not in csv_path:\n",
    "        domain = \"TF\"\n",
    "    elif 'time_' in csv_path and 'time_timefreq_' not in csv_path:\n",
    "        domain = \"T\"\n",
    "    \n",
    "    channels = int(str(str(csv_path.split(\"/\")[-1]).split(\"_\")[0]).split(\"chan\")[0])\n",
    "    \n",
    "    window_size = int(str(str(str(csv_path.split(\"/\")[-1]).split(\"_\")[-1]).split(\"-\")[0]).split(\"s\")[0])\n",
    "    step_size = str(str(str(csv_path.split(\"/\")[-1]).split(\"_\")[-1]).split(\"-\")[1]).split(\"step\")[0]\n",
    "    \n",
    "    combo_dicts = {\"VALxARO\":val * aro, \"VALxAROVAL\":val * aroval, \"AROxVALARO\":aro * valaro}\n",
    "    combo_dicts = dict(sorted(combo_dicts.items(), key=lambda item: item[1], reverse=True))    \n",
    "    \n",
    "    best_combination = list(combo_dicts.keys())[0]\n",
    "    best_acc = list(combo_dicts.values())[0]\n",
    "    \n",
    "    val_params = convert_dict_to_string(val_params)\n",
    "    aro_params = convert_dict_to_string(aro_params)\n",
    "    valaro_params = convert_dict_to_string(valaro_params)\n",
    "    aroval_params = convert_dict_to_string(aroval_params)\n",
    "    \n",
    "    time_taken = end_time\n",
    "    \n",
    "    val = round(val*100,2)\n",
    "    valstd = round(valstd*100,2)\n",
    "    aro = round(aro*100,2)\n",
    "    arostd = round(arostd*100,2)\n",
    "    valaro = round(valaro*100,2)\n",
    "    valarostd = round(valarostd*100,2)\n",
    "    aroval = round(aroval*100,2)\n",
    "    arovalstd = round(arovalstd*100,2)\n",
    "    \n",
    "    for tar in [\"VAL\", \"ARO\", \"VALARO\", \"AROVAL\"]:\n",
    "        if tar == \"VAL\": \n",
    "            mean = val\n",
    "            std = valstd\n",
    "            subs = val_sub_list\n",
    "        if tar == \"ARO\": \n",
    "            mean = aro\n",
    "            std = arostd\n",
    "            subs = aro_sub_list\n",
    "        if tar == \"VALARO\": \n",
    "            mean = valaro\n",
    "            std = valarostd\n",
    "            subs = valaro_sub_list\n",
    "        if tar == \"AROVAL\": \n",
    "            mean = aroval\n",
    "            std = arovalstd\n",
    "            subs = aroval_sub_list\n",
    "        \n",
    "        sub_results = {}\n",
    "        sub_names = []\n",
    "        subcount = 1\n",
    "        for sub in subs:\n",
    "            sub_results[f\"Subject {subcount}\"] = round(subs[subcount-1]*100,2)\n",
    "            sub_names.append(f\"Subject {subcount}\")\n",
    "            subcount += 1\n",
    "        \n",
    "        new_result = {\"Dependency\":dependency, \"Model Name\":model_name, \"Domain\":domain, \"Channels\":channels, \n",
    "                      \"Window Size\":window_size, \"Step Size\": step_size, \"Hyperparams\": \"C:200 on each\", \n",
    "                      \"Time\":time_taken, \"Target\":tar, \"Mean\":mean, \"Std\": std, \"Best Combination\": best_combination,\n",
    "                      \"Best Accuracy\": best_acc}\n",
    "        \n",
    "        new_result = {**new_result, **sub_results, **val_param_subs, **aro_param_subs, **valaro_param_subs,**aroval_param_subs}\n",
    "        \n",
    "        results = results.append(new_result, ignore_index=True)\n",
    "        \n",
    "        col_names = [\"Dependency\",\"Model Name\",\"Domain\",\"Channels\", \"Window Size\",\"Step Size\",\n",
    "                           \"Hyperparams\",\"Time\",\"Target\",\"Mean\",\"Std\",\"Best Combination\", \"Best Accuracy\"]\n",
    "        col_names = col_names+sub_names\n",
    "        col_names = col_names+list(val_param_subs.keys())+list(aro_param_subs.keys())+list(valaro_param_subs.keys())+list(aroval_param_subs.keys())\n",
    "        results = results[col_names]  \n",
    "    \n",
    "    results.to_csv(results_csv, index=False)\n",
    "    print(f\"Completed. - {dependency}, {domain}, {channels}, {window_size}-{step_size} in {time_taken}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "pregnant-leisure",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datacustom/dwt/5chan_timefreq_1s-1.0step.csv\n",
      "datacustom/dwt/5chan_timefreq_1s-1.0step.csv already exists.\n",
      "Time taken to process dataset: 0.0s.\n",
      "val\n",
      "[Estimated time for all subs: ~949.44s]\n",
      "val completed - 967.8207166194916s\n",
      "aro\n",
      "[Estimated time for all subs: ~870.4s]\n",
      "aro completed - 1059.895129442215s\n",
      "valaro\n",
      "[Estimated time for all subs: ~983.04s]\n",
      "valaro completed - 929.1666326522827s\n",
      "aroval\n",
      "[Estimated time for all subs: ~1008.64s]\n",
      "aroval completed - 991.968715429306s\n",
      "VAL: 0.7111161507464786\n",
      "ARO: 0.7032362167946139\n",
      "VALARO: 0.7433473305544105\n",
      "AROVAL: 0.7393364599139851\n",
      "VAL x ARO: 0.500082631552502\n",
      "VAL x AROVAL: 0.5257540974805612\n",
      "ARO x VALARO: 0.522748764503459\n",
      "Completed. - Dependent, TF, 5, 1-1.0 in 3949.67s\n",
      "datacustom/dwt_baseline/5chan_timefreq_1s-1.0step.csv\n",
      "datacustom/dwt_baseline/5chan_timefreq_1s-1.0step.csv already exists.\n",
      "Time taken to process dataset: 0.0s.\n",
      "val\n",
      "[Estimated time for all subs: ~875.84s]\n",
      "val completed - 861.5162041187286s\n",
      "aro\n",
      "[Estimated time for all subs: ~921.6s]\n",
      "aro completed - 921.6205081939697s\n",
      "valaro\n",
      "[Estimated time for all subs: ~962.88s]\n",
      "valaro completed - 889.6262540817261s\n",
      "aroval\n",
      "[Estimated time for all subs: ~925.76s]\n",
      "aroval completed - 877.2803285121918s\n",
      "VAL: 0.8645451244994413\n",
      "ARO: 0.8696678091155791\n",
      "VALARO: 0.8925675172998633\n",
      "AROVAL: 0.8948253268468405\n",
      "VAL x ARO: 0.7518670643049846\n",
      "VAL x AROVAL: 0.773616873604055\n",
      "ARO x VALARO: 0.7762372372579038\n",
      "Completed. - Dependent, TF, 5, 1-1.0 in 3550.75s\n",
      "datacustom/dwt/5chan_timefreq_3s-3.0step.csv\n",
      "datacustom/dwt/5chan_timefreq_3s-3.0step.csv already exists.\n",
      "Time taken to process dataset: 0.0s.\n",
      "val\n",
      "[Estimated time for all subs: ~295.68s]\n",
      "val completed - 286.73820900917053s\n",
      "aro\n",
      "[Estimated time for all subs: ~294.08s]\n",
      "aro completed - 288.8794674873352s\n",
      "valaro\n",
      "[Estimated time for all subs: ~299.52s]\n",
      "valaro completed - 283.6800653934479s\n",
      "aroval\n",
      "[Estimated time for all subs: ~297.6s]\n",
      "aroval completed - 288.138108253479s\n",
      "VAL: 0.7186744228812627\n",
      "ARO: 0.7073675767715724\n",
      "VALARO: 0.7489651488082962\n",
      "AROVAL: 0.7409874500735577\n",
      "VAL x ARO: 0.508366985001227\n",
      "VAL x AROVAL: 0.5325287280438725\n",
      "ARO x VALARO: 0.5297936623988846\n",
      "Completed. - Dependent, TF, 5, 3-3.0 in 1147.72s\n",
      "datacustom/dwt_baseline/5chan_timefreq_3s-3.0step.csv\n",
      "datacustom/dwt_baseline/5chan_timefreq_3s-3.0step.csv already exists.\n",
      "Time taken to process dataset: 0.0s.\n",
      "val\n",
      "[Estimated time for all subs: ~294.08s]\n",
      "val completed - 263.78647446632385s\n",
      "aro\n",
      "[Estimated time for all subs: ~272.32s]\n",
      "aro completed - 261.2862722873688s\n",
      "valaro\n",
      "[Estimated time for all subs: ~272.0s]\n",
      "valaro completed - 260.6575253009796s\n",
      "aroval\n",
      "[Estimated time for all subs: ~278.4s]\n",
      "aroval completed - 261.69632863998413s\n",
      "VAL: 0.8731946080731314\n",
      "ARO: 0.877986048094374\n",
      "VALARO: 0.8949855881438019\n",
      "AROVAL: 0.8995278578376781\n",
      "VAL x ARO: 0.7666526831594443\n",
      "VAL x AROVAL: 0.7854628752754348\n",
      "ARO x VALARO: 0.7857848596357957\n",
      "Completed. - Dependent, TF, 5, 3-3.0 in 1047.72s\n",
      "datacustom/dwt/5chan_timefreq_5s-5.0step.csv\n",
      "datacustom/dwt/5chan_timefreq_5s-5.0step.csv already exists.\n",
      "Time taken to process dataset: 0.0s.\n",
      "val\n",
      "[Estimated time for all subs: ~180.48s]\n",
      "val completed - 173.56422662734985s\n",
      "aro\n",
      "[Estimated time for all subs: ~184.32s]\n",
      "aro completed - 178.42078757286072s\n",
      "valaro\n",
      "[Estimated time for all subs: ~187.84s]\n",
      "valaro completed - 177.62078738212585s\n",
      "aroval\n",
      "[Estimated time for all subs: ~185.92s]\n",
      "aroval completed - 178.6549789905548s\n",
      "VAL: 0.719476242201426\n",
      "ARO: 0.7088196301247772\n",
      "VALARO: 0.7491322693850268\n",
      "AROVAL: 0.737402517825312\n",
      "VAL x ARO: 0.5099788838807794\n",
      "VAL x AROVAL: 0.5305435925148255\n",
      "ARO x VALARO: 0.5309996581000296\n",
      "Completed. - Dependent, TF, 5, 5-5.0 in 708.43s\n",
      "datacustom/dwt/5chan_timefreq_7s-7.0step.csv\n",
      "datacustom/dwt/5chan_timefreq_7s-7.0step.csv already exists.\n",
      "Time taken to process dataset: 0.0s.\n",
      "val\n",
      "[Estimated time for all subs: ~132.8s]\n",
      "val completed - 132.09986090660095s\n",
      "aro\n",
      "[Estimated time for all subs: ~136.0s]\n",
      "aro completed - 132.24391722679138s\n",
      "valaro\n",
      "[Estimated time for all subs: ~137.92s]\n",
      "valaro completed - 133.10471177101135s\n",
      "aroval\n",
      "[Estimated time for all subs: ~141.44s]\n",
      "aroval completed - 132.6085557937622s\n",
      "VAL: 0.7166411528716217\n",
      "ARO: 0.7085040118243243\n",
      "VALARO: 0.7446579391891892\n",
      "AROVAL: 0.7322054476351352\n",
      "VAL x ARO: 0.5077431318479528\n",
      "VAL x AROVAL: 0.5247285561321251\n",
      "ARO x VALARO: 0.5275931373523743\n",
      "Completed. - Dependent, TF, 5, 7-7.0 in 530.19s\n",
      "datacustom/dwt/5chan_timefreq_9s-9.0step.csv\n",
      "datacustom/dwt/5chan_timefreq_9s-9.0step.csv already exists.\n",
      "Time taken to process dataset: 0.0s.\n",
      "val\n",
      "[Estimated time for all subs: ~107.84s]\n",
      "val completed - 106.53804659843445s\n",
      "aro\n",
      "[Estimated time for all subs: ~108.8s]\n",
      "aro completed - 106.49538588523865s\n",
      "valaro\n",
      "[Estimated time for all subs: ~110.72s]\n",
      "valaro completed - 107.10133123397827s\n",
      "aroval\n",
      "[Estimated time for all subs: ~109.76s]\n",
      "aroval completed - 106.96662831306458s\n",
      "VAL: 0.7176380621693121\n",
      "ARO: 0.7081256200396825\n",
      "VALARO: 0.7383091517857143\n",
      "AROVAL: 0.7291335978835979\n",
      "VAL x ARO: 0.5081778977377204\n",
      "VAL x AROVAL: 0.5232540222477237\n",
      "ARO x VALARO: 0.522815625889231\n",
      "Completed. - Dependent, TF, 5, 9-9.0 in 427.21s\n",
      "data/dwt/32chan_timefreq_1s-1.0step.csv\n",
      "data/dwt/32chan_timefreq_1s-1.0step.csv already exists.\n",
      "Time taken to process dataset: 0.0s.\n",
      "val\n",
      "[Estimated time for all subs: ~2967.68s]\n",
      "val completed - 4171.537960767746s\n",
      "aro\n",
      "[Estimated time for all subs: ~2940.16s]\n",
      "aro completed - 3947.477289915085s\n",
      "valaro\n",
      "[Estimated time for all subs: ~3091.84s]\n",
      "valaro completed - 3426.9713699817657s\n",
      "aroval\n",
      "[Estimated time for all subs: ~2967.68s]\n",
      "aroval completed - 4022.487932443619s\n",
      "VAL: 0.6903072033898305\n",
      "ARO: 0.7034560381355932\n",
      "VALARO: 0.7297537076271187\n",
      "AROVAL: 0.7461467161016949\n",
      "VAL x ARO: 0.4856007703930713\n",
      "VAL x AROVAL: 0.5150704529106669\n",
      "ARO x VALARO: 0.5133496519821329\n",
      "Completed. - Dependent, TF, 32, 1-1.0 in 15572.12s\n",
      "data/dwt_baseline/32chan_timefreq_1s-1.0step.csv\n",
      "data/dwt_baseline/32chan_timefreq_1s-1.0step.csv already exists.\n",
      "Time taken to process dataset: 0.0s.\n",
      "val\n",
      "[Estimated time for all subs: ~2728.0s]\n",
      "val completed - 3330.5278828144073s\n",
      "aro\n",
      "[Estimated time for all subs: ~2611.52s]\n",
      "aro completed - 3179.5949082374573s\n",
      "valaro\n",
      "[Estimated time for all subs: ~2584.0s]\n",
      "valaro completed - 2655.419034719467s\n",
      "aroval\n",
      "[Estimated time for all subs: ~2618.88s]\n",
      "aroval completed - 3575.6929132938385s\n",
      "VAL: 0.9438824152542373\n",
      "ARO: 0.9493644067796609\n",
      "VALARO: 0.9520921610169492\n",
      "AROVAL: 0.956739936440678\n",
      "VAL x ARO: 0.8960883692275926\n",
      "VAL x AROVAL: 0.9030500019778127\n",
      "ARO x VALARO: 0.9038824096434214\n",
      "Completed. - Dependent, TF, 32, 1-1.0 in 12745.2s\n",
      "data/dwt/32chan_timefreq_3s-3.0step.csv\n",
      "data/dwt/32chan_timefreq_3s-3.0step.csv already exists.\n",
      "Time taken to process dataset: 0.0s.\n",
      "val\n",
      "[Estimated time for all subs: ~879.04s]\n",
      "val completed - 824.5389556884766s\n",
      "aro\n",
      "[Estimated time for all subs: ~844.48s]\n",
      "aro completed - 817.2739088535309s\n",
      "valaro\n",
      "[Estimated time for all subs: ~864.64s]\n",
      "valaro completed - 820.6126034259796s\n",
      "aroval\n",
      "[Estimated time for all subs: ~843.2s]\n",
      "aroval completed - 815.3836879730225s\n",
      "VAL: 0.7025493421052631\n",
      "ARO: 0.7104440789473684\n",
      "VALARO: 0.7386924342105263\n",
      "AROVAL: 0.7462993421052632\n",
      "VAL x ARO: 0.49912202026705327\n",
      "VAL x AROVAL: 0.5243121118096433\n",
      "ARO x VALARO: 0.5247996660480869\n",
      "Completed. - Dependent, TF, 32, 3-3.0 in 3279.27s\n",
      "data/dwt_baseline/32chan_timefreq_3s-3.0step.csv\n",
      "data/dwt_baseline/32chan_timefreq_3s-3.0step.csv already exists.\n",
      "Time taken to process dataset: 0.0s.\n",
      "val\n",
      "[Estimated time for all subs: ~776.32s]\n",
      "val completed - 790.9941976070404s\n",
      "aro\n",
      "[Estimated time for all subs: ~782.08s]\n",
      "aro completed - 785.7234342098236s\n",
      "valaro\n",
      "[Estimated time for all subs: ~778.24s]\n",
      "valaro completed - 789.8853657245636s\n",
      "aroval\n",
      "[Estimated time for all subs: ~785.28s]\n",
      "aroval completed - 784.7588772773743s\n",
      "VAL: 0.9497532894736842\n",
      "ARO: 0.9559621710526316\n",
      "VALARO: 0.9583470394736842\n",
      "AROVAL: 0.9614309210526316\n",
      "VAL x ARO: 0.9079282165696416\n",
      "VAL x AROVAL: 0.9131221798714508\n",
      "ARO x VALARO: 0.9161435164771251\n",
      "Completed. - Dependent, TF, 32, 3-3.0 in 3152.8s\n",
      "data/dwt/32chan_timefreq_5s-5.0step.csv\n",
      "data/dwt/32chan_timefreq_5s-5.0step.csv already exists.\n",
      "Time taken to process dataset: 0.0s.\n",
      "val\n",
      "[Estimated time for all subs: ~524.8s]\n",
      "val completed - 486.74261927604675s\n",
      "aro\n",
      "[Estimated time for all subs: ~500.48s]\n",
      "aro completed - 483.1696562767029s\n",
      "valaro\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Estimated time for all subs: ~520.0s]\n",
      "valaro completed - 486.5367522239685s\n",
      "aroval\n",
      "[Estimated time for all subs: ~505.6s]\n",
      "aroval completed - 484.8875389099121s\n",
      "VAL: 0.7014914772727272\n",
      "ARO: 0.7064630681818181\n",
      "VALARO: 0.7292613636363636\n",
      "AROVAL: 0.7428977272727273\n",
      "VAL x ARO: 0.495577821337487\n",
      "VAL x AROVAL: 0.521136424167097\n",
      "ARO x VALARO: 0.515196220461002\n",
      "Completed. - Dependent, TF, 32, 5-5.0 in 1942.37s\n",
      "data/dwt/32chan_timefreq_7s-7.0step.csv\n",
      "data/dwt/32chan_timefreq_7s-7.0step.csv already exists.\n",
      "Time taken to process dataset: 0.0s.\n",
      "val\n",
      "[Estimated time for all subs: ~371.2s]\n",
      "val completed - 357.19659090042114s\n",
      "aro\n",
      "[Estimated time for all subs: ~365.76s]\n",
      "aro completed - 356.30566143989563s\n",
      "valaro\n",
      "[Estimated time for all subs: ~370.88s]\n",
      "valaro completed - 357.2316801548004s\n",
      "aroval\n",
      "[Estimated time for all subs: ~368.64s]\n",
      "aroval completed - 356.9796669483185s\n",
      "VAL: 0.7023437499999999\n",
      "ARO: 0.705859375\n",
      "VALARO: 0.73193359375\n",
      "AROVAL: 0.73447265625\n",
      "VAL x ARO: 0.49575592041015615\n",
      "VAL x AROVAL: 0.5158522796630859\n",
      "ARO x VALARO: 0.5166421890258789\n",
      "Completed. - Dependent, TF, 32, 7-7.0 in 1428.49s\n",
      "data/dwt/32chan_timefreq_9s-9.0step.csv\n",
      "data/dwt/32chan_timefreq_9s-9.0step.csv already exists.\n",
      "Time taken to process dataset: 0.0s.\n",
      "val\n",
      "[Estimated time for all subs: ~283.84s]\n",
      "val completed - 275.5688798427582s\n",
      "aro\n",
      "[Estimated time for all subs: ~275.52s]\n",
      "aro completed - 273.2183372974396s\n",
      "valaro\n",
      "[Estimated time for all subs: ~286.72s]\n",
      "valaro completed - 275.26727986335754s\n",
      "aroval\n",
      "[Estimated time for all subs: ~277.44s]\n",
      "aroval completed - 274.31422448158264s\n",
      "VAL: 0.6977864583333333\n",
      "ARO: 0.7015625000000001\n",
      "VALARO: 0.733203125\n",
      "AROVAL: 0.7322916666666667\n",
      "VAL x ARO: 0.4895408121744792\n",
      "VAL x AROVAL: 0.5109832085503472\n",
      "ARO x VALARO: 0.5143878173828126\n",
      "Completed. - Dependent, TF, 32, 9-9.0 in 1098.8s\n"
     ]
    }
   ],
   "source": [
    "sample_rate = 128 \n",
    "subject_list = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','32']\n",
    "\n",
    "for channels in [[0,1,2,3,4]]:\n",
    "    for timefreq in [True]:\n",
    "        for timedomain in [False]:\n",
    "#             if timedomain == False and timefreq == False:\n",
    "#                 continue\n",
    "            for window_in_sec in [1,3,5,7,9]:\n",
    "                for baseline in [False, True]:\n",
    "                    if baseline == True and window_in_sec > 3: continue  \n",
    "                    window_size = window_in_sec * sample_rate\n",
    "                    step_size = window_size\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    data = feature_extraction(subjects=subject_list, channel=channels, window_size=window_size, step_size=step_size, timedomain=timedomain, timefreq=timefreq, baseline=baseline, directory='DEAP_5chan_custom_preproc')\n",
    "                    print(f\"Time taken to process dataset: {round(time.time()-start_time,2)}s.\")\n",
    "                    subj_dept(csv_path=data['csv_path'], verbose=True, baseline=baseline, custom=True)\n",
    "                    \n",
    "for channels in [list(range(0,32))]:\n",
    "    for timefreq in [True]:\n",
    "        for timedomain in [False]:\n",
    "#             if timedomain == False and timefreq == False:\n",
    "#                 continue\n",
    "            for window_in_sec in [1,3,5,7,9]:\n",
    "                for baseline in [False, True]:\n",
    "                    if baseline == True and window_in_sec > 3: continue  \n",
    "                    window_size = window_in_sec * sample_rate\n",
    "                    step_size = window_size\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    data = feature_extraction(subjects=subject_list, channel=channels, window_size=window_size, step_size=step_size, timedomain=timedomain, timefreq=timefreq, baseline=baseline)\n",
    "                    print(f\"Time taken to process dataset: {round(time.time()-start_time,2)}s.\")\n",
    "                    subj_dept(csv_path=data['csv_path'], verbose=True, baseline=baseline, custom=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
